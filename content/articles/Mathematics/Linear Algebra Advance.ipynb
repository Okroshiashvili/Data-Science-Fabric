{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the fourth and last post in blog series about linear algebra. \n",
    "\n",
    "1. Introduction\n",
    "2. Basics of linear algebra\n",
    "3. Intermediate linear algebra\n",
    "4. **Advances in linear algebra**\n",
    "\n",
    "First, second and third posts are [here](https://dsfabric.org/articles/mathematics/introduction.html), [here](https://dsfabric.org/articles/mathematics/basics-of-linear-algebra.html) and [here](https://dsfabric.org/articles/mathematics/intermediates-of-linear-algebra.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post I will introduce you to the advances of linear algebra, which in turn includes the following:\n",
    "* [Vector](#Vector)\n",
    "    * [Basis Vectors](#Basis_Vectors)\n",
    "* [Matrix](#Matrix)\n",
    "    * [Gaussian Elimination of a Matrix](#Gaussian_Elimination_of_a_Matrix)\n",
    "    * [Gauss-Jordan Elimination of a Matrix](#Gauss_Jordan_Elimination_of_a_Matrix)\n",
    "    * [The Inverse of a Matrix Using Gauss-Jordan Elimination](#The_Inverse_of_a_Matrix_Using_Gauss_Jordan_Elimination)\n",
    "    * [Image of a Matrix](#Image_of_a_Matrix)\n",
    "    * [Kernel of a Matrix](#Kernel_of_a_Matrix)\n",
    "    * [Rank of a Matrix](#Rank_of_a_Matrix)\n",
    "    * [Find the Basis of a Matrix](#Find_the_Basis_of_a_Matrix)\n",
    "    * [Transformations](#Transformations)\n",
    "    * [Eigenvalues](#Eigenvalues)\n",
    "    * [Eigenvectors](#Eigenvectors)\n",
    "    * [Spectrum and Spectral Radius](#Spectrum_and_Spectral_Radius)\n",
    "    * [Numerical Representation](#Numerical_Representation_Matrix)\n",
    "* [Matrix Decompositions](#Matrix_Decompositions)\n",
    "    * [Cholesky Decomposition](#Cholesky_Decomposition)\n",
    "    * [QR Decomposition](#QR_Decomposition)\n",
    "    * [Eigendecomposition](#Eigendecomposition)\n",
    "    * [Singular Value Decomposition](#Singular_Value_Decomposition)\n",
    "    * [Inverse of a Square Full Rank Matrix](#Inverse_of_a_Square_Full_Rank_Matrix)\n",
    "    * [Numerical Representation](#Numerical_Representation_Decompositions)\n",
    "* [Conclusion](#Conclusion)\n",
    "* [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector\n",
    "<a id=\"Vector\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basis Vectors\n",
    "<a id=\"Basis_Vectors\"></a>\n",
    "***\n",
    "\n",
    "In the [basics](https://dsfabric.org/articles/mathematics/basics-of-linear-algebra.html), we saw what is a unit vector. To refresh, the unit vector is the vector with length 1 and the formula is\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{X} = \\frac{X}{\\|X\\|}\n",
    "\\end{align*}\n",
    "\n",
    "For farther explanation, unit vectors can be used to represent the axes of a [Cartesian coordinate system](https://en.wikipedia.org/wiki/Cartesian_coordinate_system). For example in a three-dimensional Cartesian coordinate system such vectors are:\n",
    "\n",
    "\\begin{align*} \\hat{i} =\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "\\hat{j} =\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "\\hat{k} =\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "which represents, $x$, $y$, and $z$ axes, respectively.\n",
    "\n",
    "For two dimensional space we have\n",
    "\n",
    "\\begin{align*} \\hat{i} =\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "\\hat{j} =\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Let deal with two-dimensional space to catch the idea of basis easily and then generalize this idea for higher dimensions. Imagine, we have vector space or collection of vectors $\\vec{V}$ over the Cartesian coordinate system. This space includes all two-dimensional vectors, or in other words, vectors with only two elements, $x$, and $y$.\n",
    "\n",
    "> A **basis**, call it $B$, of vector space $V$ over the Cartesian coordinate system is a linearly independent subset of $V$ that spans whole vector space $V$. To be precise, basis $B$ to be the basis it must satisfie two conditions:\n",
    "\n",
    "* Linearly independence property - states that all vectors in $B$ are linearly independent\n",
    "\n",
    "* The spanning property - states that $B$ spans whole $V$\n",
    "\n",
    "We can combine these two conditions in one sentence. $B$ is the basis if its all elements are linearly independent and every element of $V$ is a linear combination of elements of $B$.\n",
    "\n",
    "From these conditions, we can conclude that unit vectors $\\hat{i}$ and $\\hat{j}$ are the basis of $\\mathbb{R^2}$. This kind of bases are also called **standard basis** or **natural basis**. The standard basis are denoted by $e_{1}$, $e_{2}$, $e_{3}$ and so on. I will be consistent and use the later notation for standard basis and $\\hat{i}$, $\\hat{j}$ and $\\hat{k}$ for unit vectors.\n",
    "\n",
    "These standard basis vectors are the basis in the sense that any other vector in $V$ can be expressed uniquely as a linear combination of these unit vectors. For example, every vector $v$ in two-dimensional space can be written as\n",
    "\n",
    "\\begin{align*}\n",
    "x\\ e_{1} + y\\ e_{2}\n",
    "\\end{align*}\n",
    "\n",
    "where $e_{1}$ and $e_{2}$ are unit vectors and $x$ and $y$ are scalar components or elements of the vector $v$.\n",
    "\n",
    "Now, to generalize the idea for higher dimensions we just have to apply the same logic as above, for $\\mathbb{R^3}$ and more. In $\\mathbb{R^3}$ we have standard basis vectors $e_{1}$, $e_{2}$, $e_{3}$, and generally for $\\mathbb{R^n}$ we have standard basis vector space\n",
    "\n",
    "\\begin{align*}\n",
    "E = \n",
    "\\begin{bmatrix}\n",
    "e_{1} \\\\\n",
    "e_{2} \\\\\n",
    "\\cdots \\\\\n",
    "e_{n}\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "To generalize the definition of basis further let consider the following:\n",
    "\n",
    "**If elements $\\{v_{1}, v_{2},\\cdots,v_{n}\\}$ of $V$ generate $V$ and in addition they are linearly independent, then $\\{v_{1}, v_{2},\\cdots,v_{n}\\}$ is called a basis of $V$. We shall say that the elements $v_{1}, v_{2},\\cdots,v_{n}$ constitute or form a basis of V.** Vector space $V$ can have several basis.\n",
    "\n",
    "At this stage, the notion of basis seems very abstract even for me, and believe me it was totally unclear for me until I solved some examples by hand. I'll show you how to compute basis after explaining row-echelon and reduced row-echelon forms and you'll understand it. However, it's not enough only to know how to row-reduce the given matrix. It's necessary to know which basis you want. Either column space or row space basis or the basis for nullspace. These notions are explained below and after that, we can find the basis for each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix\n",
    "<a id=\"Matrix\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Elimination of a Matrix\n",
    "<a id=\"Gaussian_Elimination_of_a_Matrix\"></a>\n",
    "***\n",
    "\n",
    "\n",
    "In linear algebra, Gaussian Elimination is the method to solve the system of linear equations. This method is the sequence of operations performed on the coefficient matrix of the system. Except for solving the linear systems, the method can be used to find the rank of a matrix, the determinant as well as the inverse of a square invertible matrix. \n",
    "\n",
    "And what is the sequence of operations?\n",
    "\n",
    "Under this notion, elementary row operations are meant. We've covered it in the previous post but for the refresher, ERO's are:\n",
    "\n",
    "* Interchange rows\n",
    "\n",
    "* Multiplay each element in a row by a non-zero number\n",
    "\n",
    "* Multiply a row by a non-zero number and add the result to another row\n",
    "\n",
    "Performing Gaussian elimination results in the matrix in **Row Echelon Form** (ref). The matrix is said to be in row echelon form if it satisfies the following conditions:\n",
    "\n",
    "* The first non-zero element in each row, called the leading entry, is a 1\n",
    "\n",
    "* Each leading entry is in a column, which is the right side of the leading entry in the previous row\n",
    "\n",
    "* Below the leading entry in a column, all other entries are zero\n",
    "\n",
    "To catch the idea of this process, let consider the example. Actually, we have no matrix (not necessarily true), we have the system of linear equations in the following form:\n",
    "\n",
    "\\begin{align*}\n",
    "\\begin{cases}\n",
    "x + 2y - z = 5\\\\\n",
    "3x + y - 2z = 9\\\\\n",
    "-x + 4y + 2z = 0\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "Based on these equations we can form the following matrix\n",
    "\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & -1 \\\\\n",
    "3 & 1 & -2 \\\\\n",
    "-1 & 4 & 2\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "This matrix is called **coefficient matrix** as it contains the coefficients of the linear equations. Having the coefficient matrix, we can rewrite our system in the following form:\n",
    "\n",
    "\\begin{align*}\n",
    "Ax = b\n",
    "\\end{align*}\n",
    "\n",
    "Where $A$ is the coefficient matrix, $x$ is the vector of the unknowns, and $b$ is the vector of the right-hand side components\n",
    "\n",
    "To solve this simultaneous system, the coefficients matrix is not enough. We need something more, on which we can perform ELO's. This matrix is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\\begin{array}{@{}ccc|c@{}}\n",
    "1 & 2 & -1 & 5 \\\\\n",
    "3 & 1 & -2 & 9 \\\\\n",
    "-1 & 4 & 2 & 0\n",
    "\\end{array}\n",
    "\\end{bmatrix} = [A | b]\n",
    "\\end{align*}\n",
    "\n",
    "which is called **augmented matrix**, which in turn gives us the possibility to perform ELO's, in other words, we do Gaussian elimination and the resulted matrix will be in row echelon form. Using back substitution on the resulted matrix gives the solution to our system of equations.\n",
    "\n",
    "Let do it by hand. We have the initial system\n",
    "\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\\begin{array}{@{}ccc|c@{}}\n",
    "1 & 2 & -1 & 5 \\\\\n",
    "3 & 1 & -2 & 9 \\\\\n",
    "-1 & 4 & 2 & 0\n",
    "\\end{array}\n",
    "\\end{bmatrix}\n",
    "\\equiv\n",
    "\\begin{cases}\n",
    "x + 2y - z = 5 \\\\\n",
    "3x + y - 2z = 9 \\\\\n",
    "-x + 4y + 2z = 0\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "Then, using ERO's\n",
    "\n",
    "1. $R3 \\rightarrow R3 + R1$\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\\begin{array}{@{}ccc|c@{}}\n",
    "1 & 2 & -1 & 5 \\\\\n",
    "3 & 1 & -2 & 9 \\\\\n",
    "0 & 6 & 1 & 5\n",
    "\\end{array}\n",
    "\\end{bmatrix}\n",
    "\\equiv\n",
    "\\begin{cases}\n",
    "x + 2y - z = 5 \\\\\n",
    "3x + y - 2z = 9 \\\\\n",
    "6y + z = 5\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "2. $R2 \\rightarrow R2 - 3R1$\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\\begin{array}{@{}ccc|c@{}}\n",
    "1 & 2 & -1 & 5 \\\\\n",
    "0 & -5 & 1 & -6 \\\\\n",
    "0 & 6 & 1 & 5\n",
    "\\end{array}\n",
    "\\end{bmatrix}\n",
    "\\equiv\n",
    "\\begin{cases}\n",
    "x + 2y - z = 5 \\\\\n",
    "-5y + z = -6 \\\\\n",
    "6y + z = 5\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "3. $R2 \\rightarrow R2 + R3$\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\\begin{array}{@{}ccc|c@{}}\n",
    "1 & 2 & -1 & 5 \\\\\n",
    "0 & 1 & 2 & -1 \\\\\n",
    "0 & 6 & 1 & 5\n",
    "\\end{array}\n",
    "\\end{bmatrix}\n",
    "\\equiv\n",
    "\\begin{cases}\n",
    "x + 2y - z = 5 \\\\\n",
    "y + 2z = -1 \\\\\n",
    "6y + z = 5\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "4. $R3 \\rightarrow R3 - 6R2$\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\\begin{array}{@{}ccc|c@{}}\n",
    "1 & 2 & -1 & 5 \\\\\n",
    "0 & 1 & 2 & -1 \\\\\n",
    "0 & 0 & -11 & 11\n",
    "\\end{array}\n",
    "\\end{bmatrix}\n",
    "\\equiv\n",
    "\\begin{cases}\n",
    "x + 2y - z = 5 \\\\\n",
    "y + 2z = -1 \\\\\n",
    "-11z = 11\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "5. $R3 \\rightarrow R3 \\cdot -\\frac{1}{11}$\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\\begin{array}{@{}ccc|c@{}}\n",
    "1 & 2 & -1 & 5 \\\\\n",
    "0 & 1 & 2 & -1 \\\\\n",
    "0 & 0 & 1 & -1\n",
    "\\end{array}\n",
    "\\end{bmatrix}\n",
    "\\equiv\n",
    "\\begin{cases}\n",
    "x + 2y - z = 5 \\quad (A)\\\\\n",
    "y + 2z = -1 \\quad (B)\\\\\n",
    "z = -1 \\quad (C)\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "6. Backsubstitution\n",
    "\\begin{align*}\n",
    "\\begin{cases}\n",
    "(C) \\quad z = -1 \\\\\n",
    "(B) \\quad y = -1 - 2z \\quad \\Rightarrow \\quad y = -1 - 2(-1) = 1 \\\\\n",
    "(A) \\quad x = 5 - 2y + z \\quad \\Rightarrow \\quad x = 5 - 2(1) + (-1) = 2\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "7. Solution\n",
    "\\begin{align*}\n",
    "x = 2 \\\\\n",
    "y = 1 \\\\\n",
    "x = -1\n",
    "\\end{align*}\n",
    "\n",
    "This is the solution of the initial system, as well as the last system and every intermediate system. The matrix obtained in step 5 above is in Row-Echelon form as it satisfied above-mentioned conditions.\n",
    "\n",
    "**Note that, starting with a particular matrix, a different sequence of ERO's can lead to different row echelon form**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gauss-Jordan Elimination of a Matrix\n",
    "<a id=\"Gauss_Jordan_Elimination_of_a_Matrix\"></a>\n",
    "***\n",
    "\n",
    "\n",
    "Gaussian elimination performs row operations to produce zeros below the main diagonal of the coefficient matrix to reduce it to row echelon form. Once it's done we perform back substitution to find the solution. However, we can continue performing ERO's to reduce coefficient matrix farther, to produce **Reduced Row Echelon Form** (rref). The matrix is in reduced row echelon form if it satisfies the following conditions:\n",
    "\n",
    "* It is in row echelon form\n",
    "\n",
    "* The leading entry in each row is the only non-zero entry in its column\n",
    "\n",
    "\n",
    "Gauss-Jordan elimination starts when Gauss elimination left off. Loosely speaking, Gaussian elimination works from the top down and when it stops, we start Gauss-Jordan elimination from the bottom up. The Reduced Row Echelon Form matrix is the result of Gauss-Jordan Elimination process.\n",
    "\n",
    "We can continue our example from step 5 and see what is Gauss-Jordan elimination. At step 5 we had\n",
    "\n",
    "5. $R3 \\rightarrow R3 \\cdot -\\frac{1}{11}$\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\\begin{array}{@{}ccc|c@{}}\n",
    "1 & 2 & -1 & 5 \\\\\n",
    "0 & 1 & 2 & -1 \\\\\n",
    "0 & 0 & 1 & -1\n",
    "\\end{array}\n",
    "\\end{bmatrix}\n",
    "\\equiv\n",
    "\\begin{cases}\n",
    "x + 2y - z = 5 \\\\\n",
    "y + 2z = -1 \\\\\n",
    "z = -1\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "Now, from bottom to up we perform the following ERO's\n",
    "\n",
    "6. $R2 \\rightarrow R2 - 2R3$\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\\begin{array}{@{}ccc|c@{}}\n",
    "1 & 2 & -1 & 5 \\\\\n",
    "0 & 1 & 0 & 1 \\\\\n",
    "0 & 0 & 1 & -1\n",
    "\\end{array}\n",
    "\\end{bmatrix}\n",
    "\\equiv\n",
    "\\begin{cases}\n",
    "x + 2y - z = 5 \\\\\n",
    "y  = 1 \\\\\n",
    "z = -1\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "7. $R1 \\rightarrow R1 + R3$\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\\begin{array}{@{}ccc|c@{}}\n",
    "1 & 2 & 0 & 4 \\\\\n",
    "0 & 1 & 0 & 1 \\\\\n",
    "0 & 0 & 1 & -1\n",
    "\\end{array}\n",
    "\\end{bmatrix}\n",
    "\\equiv\n",
    "\\begin{cases}\n",
    "x + 2y = 4 \\\\\n",
    "y  = 1 \\\\\n",
    "z = -1\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "8. $R1 \\rightarrow R1 - 2R2$\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\\begin{array}{@{}ccc|c@{}}\n",
    "1 & 0 & 0 & 2 \\\\\n",
    "0 & 1 & 0 & 1 \\\\\n",
    "0 & 0 & 1 & -1\n",
    "\\end{array}\n",
    "\\end{bmatrix}\n",
    "\\equiv\n",
    "\\begin{cases}\n",
    "x = 2 \\\\\n",
    "y = 1 \\\\\n",
    "z = -1\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "The solution is\n",
    "\n",
    "\\begin{align*}\n",
    "x = 2 \\\\\n",
    "y = 1 \\\\\n",
    "x = -1\n",
    "\\end{align*}\n",
    "\n",
    "and this is the same as the solution of the Gauss elimination. The matrix in step 8 is the Reduced Row Echelon Form of our initial coefficient matrix $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Inverse of a Matrix Using Gauss-Jordan Elimination\n",
    "<a id=\"The_Inverse_of_a_Matrix_Using_Gauss_Jordan_Elimination\"></a>\n",
    "***\n",
    "\n",
    "Suppose, we have given the matrix and want to find its inverse but do not want to use the technique mentioned in the intermediate post. We can use Gauss-Jordan elimination with little modification to find the inverse of a matrix. To be consistent, I use the above coefficient matrix but not the right-hand side of the system. So, our matrix is\n",
    "\n",
    "\\begin{align*} A =\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & -1 \\\\\n",
    "3 & 1 & -2 \\\\\n",
    "-1 & 4 & 2\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "To find the inverse of $A$, we need to augment $A$ by the identity matrix $I$ which has the same dimensions as $A$. It is a must the identity to have the same dimensions. After augmentation we have\n",
    "\n",
    "\\begin{align*} [A | I] =\n",
    "\\begin{bmatrix}\n",
    "\\begin{array}{@{}ccc|ccc@{}} \n",
    "1 & 2 & -1 & 1 & 0 & 0 \\\\\n",
    "3 & 1 & -2 & 0 & 1 & 0 \\\\\n",
    "-1 & 4 & 2 & 0 & 0 & 1\n",
    "\\end{array}\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "We have to perform elementary row operations in the same way as we did in the above example. Particularly,\n",
    "\n",
    "1. $R3 \\rightarrow R3 + R1$\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\\begin{array}{@{}ccc|ccc@{}}\n",
    "1 & 2 & -1 & 1 & 0 & 0\\\\\n",
    "3 & 1 & -2 & 0 & 1 & 0\\\\\n",
    "0 & 6 & 1 & 1 & 0 & 1\n",
    "\\end{array}\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "2. $R2 \\rightarrow R2 - 3R1$\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\\begin{array}{@{}ccc|ccc@{}}\n",
    "1 & 2 & -1 & 1 & 0 & 0\\\\\n",
    "0 & -5 & 1 & -3 & 1 & -3\\\\\n",
    "0 & 6 & 1 & 1 & 0 & 1\n",
    "\\end{array}\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "3. $R2 \\rightarrow R2 + R3$\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\\begin{array}{@{}ccc|ccc@{}}\n",
    "1 & 2 & -1 & 1 & 0 & 0\\\\\n",
    "0 & 1 & 2 & -2 & 1 & -2\\\\\n",
    "0 & 6 & 1 & 1 & 0 & 1\n",
    "\\end{array}\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "4. $R3 \\rightarrow R3 - 6R2$\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\\begin{array}{@{}ccc|ccc@{}}\n",
    "1 & 2 & -1 & 1 & 0 & 0\\\\\n",
    "0 & 1 & 2 & -2 & 1 & -2\\\\\n",
    "0 & 0 & -11 & 13 & -6 & 13\n",
    "\\end{array}\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "5. $R3 \\rightarrow R3 \\cdot -\\frac{1}{11}$\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\\begin{array}{@{}ccc|ccc@{}}\n",
    "1 & 2 & -1 & 1 & 0 & 0\\\\\n",
    "0 & 1 & 2 & -2 & 1 & -2\\\\\n",
    "0 & 0 & 1 & -\\frac{13}{11} & \\frac{6}{11} & -\\frac{13}{11}\n",
    "\\end{array}\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "6. $R2 \\rightarrow R2 - 2R3$\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\\begin{array}{@{}ccc|ccc@{}}\n",
    "1 & 2 & -1 & 1 & 0 & 0\\\\\n",
    "0 & 1 & 0 & \\frac{4}{11} & -\\frac{9}{100} & \\frac{4}{11}\\\\\n",
    "0 & 0 & 1 & -\\frac{13}{11} & \\frac{6}{11} & -\\frac{13}{11}\n",
    "\\end{array}\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "7. $R1 \\rightarrow R1 + R3$\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\\begin{array}{@{}ccc|ccc@{}}\n",
    "1 & 2 & 0 & -\\frac{2}{11} & \\frac{6}{11} & -\\frac{13}{11}\\\\\n",
    "0 & 1 & 0 & \\frac{4}{11} & -\\frac{9}{100} & \\frac{4}{11}\\\\\n",
    "0 & 0 & 1 & -\\frac{13}{11} & \\frac{6}{11} & -\\frac{13}{11}\n",
    "\\end{array}\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "8. $R1 \\rightarrow R1 - 2R2$\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\\begin{array}{@{}ccc|ccc@{}}\n",
    "1 & 0 & 0 & -\\frac{10}{11} & \\frac{18}{25} & -\\frac{21}{11}\\\\\n",
    "0 & 1 & 0 & \\frac{4}{11} & -\\frac{9}{100} & \\frac{4}{11}\\\\\n",
    "0 & 0 & 1 & -\\frac{13}{11} & \\frac{6}{11} & -\\frac{13}{11}\n",
    "\\end{array}\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Our inverse of $A$ is\n",
    "\n",
    "\\begin{align*}A^{-1} =\n",
    "\\begin{bmatrix}\n",
    "-\\frac{10}{11} & \\frac{18}{25} & -\\frac{21}{11}\\\\\n",
    "\\frac{4}{11} & -\\frac{9}{100} & \\frac{4}{11}\\\\\n",
    "-\\frac{13}{11} & \\frac{6}{11} & -\\frac{13}{11}\n",
    "\\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image of a Matrix\n",
    "<a id=\"Image_of_a_Matrix\"></a>\n",
    "***\n",
    "\n",
    "Let $A$ be $m\\times n$ matrix. Space spanned by its column vectors are called range, image, or column space of a matrix $A$. The row space is defined similarly. I only consider column space as all the logic is the same for row space.\n",
    "\n",
    "The precise definition is the following:\n",
    "\n",
    "Let $A$ be an $m\\times n$ matrix, with column vectors $v_{1}, v_{2}, \\cdots, v_{n}$. A linear combination of these vectors is any vector of the following form: $c_{1}v_{1} + c_{2}v_{2} + \\cdots + c_{n}v_{n}$, where $c_{1}, c_{2}, \\cdots , c_{n}$ are scalars. The set of all possible linear combinations of $v_{1}, v_{2}, \\cdots , v_{n}$ is called the column space of $A$.\n",
    "\n",
    "For example:\n",
    "\n",
    "\\begin{align*}A =\n",
    "\\begin{bmatrix}\n",
    "1 & 0\\\\\n",
    "0 & 1\\\\\n",
    "2 & 0\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Column vectors are:\n",
    "\n",
    "\\begin{align*}v_{1} =\n",
    "\\begin{bmatrix}\n",
    "1\\\\\n",
    "0\\\\\n",
    "2\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "v_{2} =\n",
    "\\begin{bmatrix}\n",
    "0\\\\\n",
    "1\\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "A linear combination of $v_{1}$ and $v_{2}$ is any vector of the form\n",
    "\n",
    "\\begin{align*}c_{1}\n",
    "\\begin{bmatrix}\n",
    "1\\\\\n",
    "0\\\\\n",
    "2\n",
    "\\end{bmatrix} + c_{2}\n",
    "\\begin{bmatrix}\n",
    "0\\\\\n",
    "1\\\\\n",
    "0\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "c_{1}\\\\\n",
    "c_{2}\\\\\n",
    "2c_{1}\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "The set of all such vectors is the column space of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel of a Matrix\n",
    "<a id=\"Kernel_of_a_Matrix\"></a>\n",
    "***\n",
    "\n",
    "In linear algebra, the kernel or a.k.a null space is the solution of the following homogeneous system:\n",
    "\n",
    "$A\\cdot X = 0$\n",
    "\n",
    "where $A$ is a $m\\times n$ matrix and $X$ is a $m\\times 1$ vector and is denoted by $Ker(A)$.\n",
    "\n",
    "For more clarity, let consider the numerical example. Lat our matrix $A$ be the following:\n",
    "\n",
    "\\begin{align*}A =\n",
    "\\begin{bmatrix}\n",
    "2 & 7 & 1 & 3 \\\\\n",
    "-4 & -2 & 2 & -2 \\\\\n",
    "-1 & 7 & 3 & 2 \\\\\n",
    "-2 & 2 & 2 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "and our $X$ is\n",
    "\n",
    "\\begin{align*}X =\n",
    "\\begin{bmatrix}\n",
    "x_{1} \\\\\n",
    "x_{2} \\\\\n",
    "x_{3} \\\\\n",
    "x_{4} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "We have to form the following system:\n",
    "\n",
    "\\begin{align*}A\\cdot X =\n",
    "\\begin{bmatrix}\n",
    "2 & 7 & 1 & 3 \\\\\n",
    "-4 & -2 & 2 & -2 \\\\\n",
    "-1 & 7 & 3 & 2 \\\\\n",
    "-2 & 2 & 2 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "x_{1} \\\\\n",
    "x_{2} \\\\\n",
    "x_{3} \\\\\n",
    "x_{4} \\\\\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "After that, we have to put this system into row-echelon or reduced row-echelon form. Let skip detailed calculation and present only results, which is the last matrix.\n",
    "\n",
    "\\begin{align*}A =\n",
    "\\begin{bmatrix}\n",
    "2 & 7 & 1 & 3\\\\\n",
    "-4 & -2 & 2 & -2\\\\\n",
    "-1 & 7 & 3 & 2\\\\\n",
    "-2 & 2 & 2 & 0\n",
    "\\end{bmatrix}\n",
    "\\rightarrow\n",
    "\\begin{bmatrix}\n",
    "-1 & 7 & 3 & 2\\\\\n",
    "-4 & -2 & 2 & -2\\\\\n",
    "2 & 7 & 1 & 3\\\\\n",
    "-2 & 2 & 2 & 0\n",
    "\\end{bmatrix} \n",
    "\\rightarrow\n",
    "\\begin{bmatrix}\n",
    "-1 & 7 & 3 & 2\\\\\n",
    "0 & -30 & -10 & -10\\\\\n",
    "0 & 21 & 7 & 7\\\\\n",
    "0 & -12 & -4 & -4\n",
    "\\end{bmatrix}\n",
    "\\rightarrow\n",
    "\\begin{bmatrix}\n",
    "-1 & 7 & 3 & 2\\\\\n",
    "0 & 3 & 1 & 1\\\\\n",
    "0 & 3 & 1 & 1\\\\\n",
    "0 & 3 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "\\rightarrow\n",
    "\\begin{bmatrix}\n",
    "-1 & 7 & 3 & 2\\\\\n",
    "0 & 3 & 1 & 1\\\\\n",
    "0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Now, to find the kernel of the original matrix $A$, we have to solve the following system of equations:\n",
    "\n",
    "\\begin{align*}\n",
    "\\begin{cases}\n",
    "x_{1} - 7x_{2} - 3x_{3} - 2x_{4} = 0 \\\\\n",
    "        3x_{2} + x_{3} + x_{4} = 0\n",
    "\\end{cases}\n",
    "\\rightarrow\n",
    "\\begin{cases}\n",
    "x_{1} = \\frac{2}{3}x_{3} - \\frac{1}{3}x_{4}\\\\\n",
    "x_{2} = -\\frac{1}{3}x_{3} - \\frac{1}{3}x_{4}\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "From this solution we conclude that the kernel of $A$ is\n",
    "\n",
    "\\begin{align*}Ker(A) = \n",
    "\\begin{bmatrix}\n",
    "x_{1} = \\frac{2}{3}x_{3} - \\frac{1}{3}x_{4} \\\\\n",
    "x_{2} = -\\frac{1}{3}x_{3} - \\frac{1}{3}x_{4} \\\\\n",
    "x_{3} \\\\\n",
    "x_{4}\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Where, $x_{3}$ and $x_{4}$ are free variables and can be any number in $R$\n",
    "\n",
    "Note, that both original matrix $A$ and its row-echelon for has the same kernel. This means that row reduction preserves the kernel or null space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank of a Matrix\n",
    "<a id=\"Rank_of_a_Matrix\"></a>\n",
    "***\n",
    "\n",
    "In the intermediate tutorial, you saw how to calculate the determinant of a matrix and also saw that any non zero determinant of sub-matrix of the original matrix shows its nondegenerateness. In other words, nonzero determinant gives us information about the rank of the matrix. Also, I said that there was not only one way to find the rank of a matrix. After reviewing Gauss and Gauss-Jordan Elimination and Row-Echelon and Reduced Row-Echelon forms you know that indeed there are other ways to find the determinant of a matrix as well as the rank of the matrix. To keep [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself), here I only consider a numerical example. The code is provided in the intermediate tutorial.\n",
    "\n",
    "Suppose we have matrix $A$ in the following form:\n",
    "\n",
    "\\begin{align*}A =\n",
    "\\begin{bmatrix}\n",
    "3 & 2 & -1\\\\\n",
    "2 & -3 & -5\\\\\n",
    "-1 & -4 &- 3\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Perform Elementary Row Operations we get reduced-echelon form:\n",
    "\n",
    "\\begin{align*}A = \n",
    "\\begin{bmatrix}\n",
    "3 & 2 & -1\\\\\n",
    "2 & -3 & -5\\\\\n",
    "-1 & -4 & -3\n",
    "\\end{bmatrix}\n",
    "\\rightarrow\n",
    "\\begin{bmatrix}\n",
    "1 & 4 & 3\\\\\n",
    "3 & 2 & -1\\\\\n",
    "2 & -3 & -5\n",
    "\\end{bmatrix}\n",
    "\\rightarrow\n",
    "\\begin{bmatrix}\n",
    "1 & 4 & 3\\\\\n",
    "0 & -10 & -10\\\\\n",
    "0 & -11 & -11\n",
    "\\end{bmatrix}\n",
    "\\rightarrow\n",
    "\\begin{bmatrix}\n",
    "1 & 4 & 3\\\\\n",
    "0 & 1 & 1\\\\\n",
    "0 & -11 & -11\n",
    "\\end{bmatrix}\n",
    "\\rightarrow\n",
    "\\begin{bmatrix}\n",
    "1 & 4 & 3\\\\\n",
    "0 & 1 & 1\\\\\n",
    "0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "From the last matrix we see that the nonzero determinant only exists in $2\\times2$ sub-matrices, hence rank of the matrix $A$ is 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the Basis of a Matrix\n",
    "<a id=\"Find_the_Basis_of_a_Matrix\"></a>\n",
    "***\n",
    "\n",
    "Now we are able to find the basis for column space and row space as well as the basis for the kernel. The columns of a matrix $A$ span the column space but they may not form a basis if the column vectors are linearly dependent. If this is the case, only some subset of these vectors forms the basis. To find the basis for column space we reduce matrix $A$ to reduced row-echelon form.\n",
    "\n",
    "For example:\n",
    "\n",
    "\\begin{align*}A =\n",
    "\\begin{bmatrix}\n",
    "2 & 7 & 1 & 3 \\\\\n",
    "-4 & -2 & 2 & -2 \\\\\n",
    "-1 & 7 & 3 & 2 \\\\\n",
    "-2 & 2 & 2 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Row reduced form of $A$ is:\n",
    "\n",
    "\\begin{align*}B =\n",
    "\\begin{bmatrix}\n",
    "-1 & 7 & 3 & 2\\\\\n",
    "0 & 3 & 1 & 1\\\\\n",
    "0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "We see that only column 1 and column 2 are linearly independent in reduced form and hence column 1 and column 2 of the original matrix $A$ form the basis, which is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\n",
    "2 \\\\-4\\\\-1\\\\-2\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "\\text{and}\n",
    "\\quad\n",
    "\\begin{bmatrix}\n",
    "7\\\\-2 \\\\ 7 \\\\ 2\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "To find the basis for row space, let consider different matrix and again let it be $A$.\n",
    "\n",
    "\\begin{align*}A =\n",
    "\\begin{bmatrix}\n",
    "1 & 3 & 2 \\\\\n",
    "2 & 7 & 4 \\\\\n",
    "1 & 5 & 2\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "To reduce $A$ to reduced row-echelon form we have:\n",
    "\n",
    "\\begin{align*}B =\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 2 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "As in column space case, we see that linearly independent, nonzero row vectors are\n",
    "\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\n",
    "1 \\\\0\\\\2\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "\\text{and}\n",
    "\\quad\n",
    "\\begin{bmatrix}\n",
    "0\\\\1 \\\\ 0\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "To find the basis for kernel let consider our old example. In this case our matrix is:\n",
    "\n",
    "\\begin{align*}A =\n",
    "\\begin{bmatrix}\n",
    "2 & 7 & 1 & 3 \\\\\n",
    "-4 & -2 & 2 & -2 \\\\\n",
    "-1 & 7 & 3 & 2 \\\\\n",
    "-2 & 2 & 2 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "And its row reduced form is\n",
    "\n",
    "\\begin{align*}B =\n",
    "\\begin{bmatrix}\n",
    "-1 & 7 & 3 & 2\\\\\n",
    "0 & 3 & 1 & 1\\\\\n",
    "0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "We solved this and got the following result:\n",
    "\n",
    "\\begin{align*}Ker(A) = \n",
    "\\begin{bmatrix}\n",
    "x_{1} = \\frac{2}{3}x_{3} - \\frac{1}{3}x_{4} \\\\\n",
    "x_{2} = -\\frac{1}{3}x_{3} - \\frac{1}{3}x_{4} \\\\\n",
    "x_{3} \\\\\n",
    "x_{4}\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Now to have basis for null space just plug values for $x_{3} = 1$ and $x_{4} = 0$, resulted vector is\n",
    "\n",
    "\\begin{align*} \n",
    "\\begin{bmatrix}\n",
    "\\frac{2}{3} \\\\\n",
    "-\\frac{1}{3} \\\\\n",
    "1 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "The resulted vector is one set of the basis for kernel space. The values for $x_{3}$ and $x_{4}$ are up to you as they are free variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations\n",
    "<a id=\"Transformations\"></a>\n",
    "***\n",
    "\n",
    "Matrices and vectors are used together to manipulate spatial dimensions. This has a lot of applications, including the mathematical generation of 3D computer graphics, geometric modeling, and the training and optimization of machine learning algorithms. Here, I present some types of transformation. However, the list will not be exhaustive. Firstly, define linear transformation:\n",
    "\n",
    "> Linear transformation or linear map, is a mapping (function) between two vector spaces that preserves addition and scalar multiplication operations\n",
    "\n",
    "#### Linear Transformation\n",
    "\n",
    "You can manipulate a vector by multiplying it with a matrix. The matrix acts like a function that operates on an input vector to produce a vector output. Specifically, matrix multiplications of vectors are *linear transformations* that transform the input vector into the output vector.\n",
    "\n",
    "For example, consider a matrix $A$ and vector $v$\n",
    "\n",
    "\\begin{align*}A =\n",
    "\\begin{bmatrix}\n",
    "2 & 3 \\\\\n",
    "5 & 2\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "\\vec{v} =\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "2\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Define transformation $T$ to be:\n",
    "\n",
    "\\begin{align*}\n",
    "T(\\vec{v}) = A \\vec{v}\n",
    "\\end{align*}\n",
    "\n",
    "This transformation is simply dot or inner product and give the following result:\n",
    "\n",
    "\\begin{align*}\n",
    "T(\\vec{v}) = A \\vec{v} =\n",
    "\\begin{bmatrix}\n",
    "8 \\\\\n",
    "9\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "In this case, both the input and output vector has 2 components. In other words, the transformation takes a 2-dimensional vector and produces a new 2-dimensional vector. Formally we can write this in the following way:\n",
    "\n",
    "\\begin{align*}\n",
    "T: \\rm I\\!R^{2} \\to \\rm I\\!R^{2}\n",
    "\\end{align*}\n",
    "\n",
    "The transformation does not necessarily have to be $n \\times n$. The dimension of the output vector and the input vector may differ. Rewrite our matrix $A$ and vector $v$.\n",
    "\n",
    "\\begin{align*}A =\n",
    "\\begin{bmatrix}\n",
    "2 & 3 \\\\\n",
    "5 & 2 \\\\\n",
    "1 & 1\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "\\vec{v} =\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "2\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Apply above transformation gives,\n",
    "\n",
    "\\begin{align*}\n",
    "T(\\vec{v}) = A \\vec{v} =\n",
    "\\begin{bmatrix}\n",
    "8 \\\\\n",
    "9 \\\\\n",
    "3\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Now, our transformation transforms a vector from 2-dimensional space into 3-dimensional space. We can rite this transformation as\n",
    "\n",
    "\\begin{align*}\n",
    "T: \\rm I\\!R^{2} \\to \\rm I\\!R^{3}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "#### Transformations of Magnitude and Aplitude\n",
    "\n",
    "When we multiply a vector by a matrix we transform it in at least one of the following two ways\n",
    "\n",
    "* Scale the length (Magnitude)\n",
    "\n",
    "* Change the direction (Aplitude)\n",
    "\n",
    "*Change in length (Magnitude), but not change in direction (Amplitude)*\n",
    "\n",
    "\\begin{align*}A =\n",
    "\\begin{bmatrix}\n",
    "2 & 0 \\\\\n",
    "0 & 2\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "\\vec{v} =\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "transformation gives,\n",
    "\n",
    "\\begin{align*}\n",
    "T(\\vec{v}) = A \\vec{v} =\n",
    "\\begin{bmatrix}\n",
    "2 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "In this case, the resulted vector changed in length but not changed in direction. See code and visualization in Numerical Representation part.\n",
    "\n",
    "*Change in direction (Amplitude), but not change in length (Magnitude)*\n",
    "\n",
    "\\begin{align*}A =\n",
    "\\begin{bmatrix}\n",
    "0 & -1 \\\\\n",
    "1 & 0\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "\\vec{v} =\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "transformation gives,\n",
    "\n",
    "\\begin{align*}\n",
    "T(\\vec{v}) = A \\vec{v} =\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "This time, resulted vector changed in direction but has the same length.\n",
    "\n",
    "*Change in direction (Amplitude) and in length (Magnitude)*\n",
    "\n",
    "\\begin{align*}A =\n",
    "\\begin{bmatrix}\n",
    "2 & 1 \\\\\n",
    "1 & 2\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "\\vec{v} =\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "transformation gives,\n",
    "\n",
    "\\begin{align*}\n",
    "T(\\vec{v}) = A \\vec{v} =\n",
    "\\begin{bmatrix}\n",
    "2 \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "This time the resulted vector changed in the direction as well as the length.\n",
    "\n",
    "#### Affine Transformation\n",
    "\n",
    "An Affine transformation multiplies a vector by a matrix and adds an offset vector, sometimes referred to as *bias*.\n",
    "\n",
    "\\begin{align*}\n",
    "T(\\vec{v}) = A\\vec{v} + \\vec{b}\n",
    "\\end{align*}\n",
    "\n",
    "Consider following example\n",
    "\n",
    "\\begin{align*}T(\\vec{v}) = A\\vec{v} + \\vec{b} = \n",
    "\\begin{bmatrix}\n",
    "5 & 2\\\\\n",
    "3 & 1\n",
    "\\end{bmatrix} \\cdot\n",
    "\\begin{bmatrix}\n",
    "1\\\\\n",
    "1\n",
    "\\end{bmatrix} + \n",
    "\\begin{bmatrix}\n",
    "-2\\\\\n",
    "-6\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "5\\\\\n",
    "-2\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "This kind of transformation is actually the basic block of linear regression, which is a core foundation for machine learning. However, these concepts are out of the scope of this tutorial. Python code is below for this transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalues\n",
    "<a id=\"Eigenvalues\"></a>\n",
    "***\n",
    "\n",
    "Let consider matrix $A$\n",
    "\n",
    "\\begin{align*}A =\n",
    "\\begin{bmatrix}\n",
    "2 & 0 & 0 \\\\\n",
    "0 & 3 & 4 \\\\\n",
    "0 & 4 & 9\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Now, let multiply this matrix with vector\n",
    "\\begin{align*}\n",
    "\\vec{v} =\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "We have the following:\n",
    "\n",
    "\\begin{align*}A \\cdot v =\n",
    "\\begin{bmatrix}\n",
    "2 & 0 & 0 \\\\\n",
    "0 & 3 & 4 \\\\\n",
    "0 & 4 & 9\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "2 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix} =\n",
    "2 \\cdot v\n",
    "\\end{align*}\n",
    "\n",
    "That's the beautiful relationship yes? To prove this is not the only one vector, which can do this try this vector $\\vec{v} = [0\\quad 1\\quad 2]$ instead of old $v$. You should get $11\\cdot \\vec{v}$\n",
    "\n",
    "This beautiful relationship comes from the notion of eigenvalues and eigenvectors. In this case $2$ and $11$ are eigenvalues of the matrix $A$.\n",
    "\n",
    "Let formalize the notion of eigenvalue and eigenvector:\n",
    "\n",
    "> Let $A$ be an $n\\times n$ **square** matrix. If $\\lambda$ is a scalar and $v$ is non-zero vector in $\\mathbb{R^n}$ such that $$Av = \\lambda v$$ then we say that $\\lambda$ is an *eigenvalue* and $v$ is *eigenvector*\n",
    "\n",
    "I believe you are interested in how to find eigenvalues. Consider again our matrix $A$ and follow steps to find eigenvalues. Given that our matrix $A$ is a square matrix, the condition that characterizes an eigenvalue $\\lambda$ is the existence of a nonzero vector $v$ such that $Av = \\lambda v$. We can rewrite this equation in the following way:\n",
    "\n",
    "\\begin{align*}\n",
    "Av = \\lambda v\n",
    "\\\\\n",
    "Av - \\lambda v = 0\n",
    "\\\\\n",
    "Av - \\lambda I v = 0\n",
    "\\\\\n",
    "(A - \\lambda I)v = 0\n",
    "\\end{align*}\n",
    "\n",
    "The final form of this equation makes it clear that $v$ is the solution of a square, homogeneous system. To have the nonzero solution(we required it in above definition), then the determinant of the **coefficient matrix** - $(A - \\lambda I)$ must be zero. This is achieved when the columns of the coefficient matrix are linearly dependent. In other words, to find eigenvalues we have to choose $\\lambda$ such that to solve the following equation:\n",
    "\n",
    "\\begin{align*}\n",
    "det(A - \\lambda I) = 0\n",
    "\\end{align*}\n",
    "\n",
    "This equation is called **characteristic equation**\n",
    "\n",
    "For more clarity, let solve it with a particular example. We have square matrix $A$ and follow the above equation gives us:\n",
    "\n",
    "\\begin{align*}det(A - \\lambda I) = det\\Bigg(\n",
    "\\begin{bmatrix}\n",
    "2 & 0 & 0 \\\\\n",
    "0 & 3 & 4 \\\\\n",
    "0 & 4 & 9\n",
    "\\end{bmatrix} -\n",
    "\\lambda \\cdot\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\Bigg) =\n",
    "det\\Bigg(\n",
    "\\begin{bmatrix}\n",
    "2 - \\lambda & 0 & 0 \\\\\n",
    "0 & 3 - \\lambda & 4 \\\\\n",
    "0 & 4 & 9 - \\lambda\n",
    "\\end{bmatrix}\n",
    "\\Bigg)\n",
    "\\Rightarrow\n",
    "\\\\\n",
    "\\\\\n",
    "\\Rightarrow\n",
    "(2 - \\lambda)[(3 - \\lambda)(9 - \\lambda) - 16] =\n",
    "-\\lambda^3 + 14\\lambda^2 - 35\\lambda + 22\n",
    "\\end{align*}\n",
    "\n",
    "The equation $-\\lambda^3 + 14\\lambda^2 - 35\\lambda + 22$ is called  **characteristic polynomial** of the matrix $A$ and will be of degree $n$ if $A$ is $n\\times n$\n",
    "\n",
    "The zeros or roots of this characteristic polynomial are the eigenvalues of the original matrix $A$. In this case the roots are $2$, $1$, and $11$. Surprise! Our matrix $A$ have three eigenvalues and two of them are already known for us from above example.\n",
    "\n",
    "Eigenvalues of a square matrix $A$ have some nice features:\n",
    "\n",
    "* The determinant of $A$ eqauls to the product of the eigenvalues\n",
    "\n",
    "* The trace of $A$ (The sum of the elements on the principal diagonal) equal the sum of the eigenvalues\n",
    "\n",
    "* If $A$ is symmetric matrix, then all of its eigenvalues are real\n",
    "\n",
    "* If $A$ is invertible (The determiannt of $A$ is not zero) and $\\lambda_{1}, \\cdots, \\lambda_{n}$ are its eigenvalues, then the eigenvalues of $A^{-1}$ are $1 / \\lambda_{1}, \\cdots, 1 / \\lambda_{n}$\n",
    "\n",
    "From first feature we have that the matrix is invertible if and only if all its eigenvalues are nonzero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvectors\n",
    "<a id=\"Eigenvectors\"></a>\n",
    "***\n",
    "\n",
    "It's time to calculate eigenvectors, but let firstly define what is eigenvector and how it relates to the eigenvalue.\n",
    "> Any nonzero vector $v$ which satisfies characteristic equation is said to be an eigenvector of $A$ corresponding to $\\lambda$\n",
    "\n",
    "Continue above example and see what are eigenvectors corresponding to eigenvalues $\\lambda = 2$, $\\lambda = 1$, and $\\lambda = 11$, repectively.\n",
    "\n",
    "Eigenvector for $\\lambda = 1$\n",
    "\n",
    "\n",
    "\\begin{align*}(A - 1I)\\cdot\n",
    "\\begin{bmatrix}\n",
    "v_{1} \\\\\n",
    "v_{2} \\\\\n",
    "v_{3}\n",
    "\\end{bmatrix} =\\Bigg(\n",
    "\\begin{bmatrix}\n",
    "2 & 0 & 0 \\\\\n",
    "0 & 3 & 4 \\\\\n",
    "0 & 4 & 9\n",
    "\\end{bmatrix} -\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\\Bigg)\\cdot\n",
    "\\begin{bmatrix}\n",
    "v_{1} \\\\\n",
    "v_{2} \\\\\n",
    "v_{3}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 2 & 4 \\\\\n",
    "0 & 4 & 8\n",
    "\\end{bmatrix}\\cdot\n",
    "\\begin{bmatrix}\n",
    "v_{1} \\\\\n",
    "v_{2} \\\\\n",
    "v_{3}\n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Rewrite this as a system of equations, we'll get\n",
    "\n",
    "\\begin{align*}\n",
    "\\begin{cases}\n",
    "v_{1} = 0\\\\\n",
    "2v_{2} + 4v_{3} = 0\\\\\n",
    "4v_{2} + 8v{3} = 0\n",
    "\\end{cases}\\rightarrow\n",
    "\\begin{cases}\n",
    "v_{1} = 0 \\\\\n",
    "v_{2} = -2v_{3}\\\\\n",
    "v_{3} = 1\n",
    "\\end{cases}\n",
    "\\rightarrow\n",
    "\\begin{cases}\n",
    "v_{1} = 0 \\\\\n",
    "v_{2} = -2\\\\\n",
    "v_{3} = 1\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "So, our eigenvector correcponding to eigenvalue $\\lambda = 1$ is\n",
    "\n",
    "\\begin{align*} v_{\\lambda = 1} =\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "-2 \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Finding eigenvectors for $\\lambda = 2$ and $\\lambda = 11$ is up to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectrum and Spectral Radius\n",
    "<a id=\"Spectrum_and_Spectral_Radius\"></a>\n",
    "***\n",
    "\n",
    "The **Spectral Radius** of a sqaure matrix $A$ is the largest absolute values of its eigenvalues and is denoted by $\\rho(A)$. More formally,\n",
    "> Spectral radius of a $n \\times n$ matrix $A$ is:\n",
    "\\begin{equation}\n",
    "\\rho(A) = max\n",
    "\\Big\\{\n",
    "\\mid \\lambda\n",
    "\\mid \\ :\n",
    "\\lambda \\ is \\ an \\ eigenvalue \\ of \\ A\n",
    "\\Big\\}\n",
    "\\end{equation}\n",
    "Stated otherwise, we have\n",
    "\\begin{equation}\n",
    "\\rho(A) = max\n",
    "\\Big\\{\n",
    "\\mid \\lambda_{1}\n",
    "\\mid,\n",
    "\\cdots,\n",
    "\\mid \\lambda_{n}\n",
    "\\mid\n",
    "\\Big\\}\n",
    "\\end{equation}\n",
    "\n",
    "It's noteworthy that the set of all eigenvalues\n",
    "\\begin{align*}\n",
    "\\Big\\{ \\lambda : \\lambda \\in \\lambda(A)\n",
    "\\Big\\}\n",
    "\\end{align*}\n",
    "is called the **Spectrum**\n",
    "\n",
    "From above example we had three eivenvalues, $\\lambda = 2$, $\\lambda = 1$ and $\\lambda = 11$ which are spectrum of $A$ and spectral radius for our matrix $A$ is $\\lambda = 11$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Representation\n",
    "<a id=\"Numerical_Representation_Matrix\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel or Null Space of a Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from scipy.linalg import null_space\n",
    "\n",
    "\n",
    "A = np.array([[2,7,1,3], [-4,-2,2,-2], [-1,7,3,2],[-2,2,2,0]])\n",
    "\n",
    "\n",
    "kernel_A = null_space(A)\n",
    "print(\"Normilized Kernel\", kernel_A, sep='\\n') # This matrix is normilized, meaning that it has unit length\n",
    "\n",
    "\n",
    "# To find unnormilized kernel we have to do the following:\n",
    "\n",
    "\n",
    "# Import sympy\n",
    "from sympy import Matrix\n",
    "\n",
    "\n",
    "B = [[2,7,1,3], [-4,-2,2,-2], [-1,7,3,2],[-2,2,2,0]]\n",
    "\n",
    "B = Matrix(B)\n",
    "\n",
    "kernel_B = B.nullspace()\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Unnormiled Kernel\", kernel_B, sep='\\n')\n",
    "\n",
    "\n",
    "# In unnormilized case, we clearly see that sympy automaticaly choose values for our free variables. \n",
    "# In first case x_3 = 1; x_4 = 0 and in the second case x_3 = 0; x_4 = 1\n",
    "# Resulted vector(s) are basis for the null space for our matrix A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "v = np.array([1,0])\n",
    "\n",
    "A = np.array([[2,0],\n",
    "              [0,2]])\n",
    "\n",
    "t = A@v # dot product\n",
    "print (\"Resulted vector is: t =\", t)\n",
    "\n",
    "# Plot v and t\n",
    "vecs = np.array([t,v])\n",
    "origin = [0], [0]\n",
    "plt.axis('equal')\n",
    "plt.grid()\n",
    "plt.quiver(*origin, vecs[:,0], vecs[:,1], color=['blue', 'green'], scale=10)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Original vector v is green and transformed vector t is blue.\n",
    "# Vector t has same direction as v but greater magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "v = np.array([1,0])\n",
    "\n",
    "A = np.array([[0,-1],\n",
    "              [1,0]])\n",
    "\n",
    "t = A@v\n",
    "print (\"Resulted vector is: t =\", t)\n",
    "\n",
    "# Plot v and t\n",
    "vecs = np.array([v,t])\n",
    "origin = [0], [0]\n",
    "plt.axis('equal')\n",
    "plt.grid()\n",
    "plt.quiver(*origin, vecs[:,0], vecs[:,1], color=['green', 'blue'], scale=10)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Resulted vector change the direction but has the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "v = np.array([1,0])\n",
    "A = np.array([[2,1],\n",
    "              [1,2]])\n",
    "\n",
    "t = A@v\n",
    "print (\"Resulted vector is: t =\", t)\n",
    "\n",
    "# Plot v and t\n",
    "vecs = np.array([v,t])\n",
    "origin = [0], [0]\n",
    "plt.axis('equal')\n",
    "plt.grid()\n",
    "plt.quiver(*origin, vecs[:,0], vecs[:,1], color=['green', 'blue'], scale=10)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Resulted vector changed the direction, as well as the length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "v = np.array([1,1])\n",
    "\n",
    "A = np.array([[5,2],\n",
    "              [3,1]])\n",
    "\n",
    "b = np.array([-2,-6])\n",
    "\n",
    "t = A@v + b\n",
    "print (\"Resulted vector is: t =\", t)\n",
    "\n",
    "# Plot v and t\n",
    "vecs = np.array([v,t])\n",
    "origin = [0], [0]\n",
    "plt.axis('equal')\n",
    "plt.grid()\n",
    "plt.quiver(*origin, vecs[:,0], vecs[:,1], color=['green', 'blue'], scale=15)\n",
    "plt.show()\n",
    "\n",
    "# The resulted vector t is blue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eigenvalues and Eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "A = np.array([[2,0,0],\n",
    "              [0,3,4],\n",
    "              [0,4,9]])\n",
    "\n",
    "\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "\n",
    "\n",
    "print(\"Eigenvalues are: \", eigenvalues)\n",
    "print()\n",
    "print(\"Eigenvectors are: \", eigenvectors, sep='\\n')\n",
    "\n",
    "\n",
    "# Note that this eigenvectors seems different from my calculation. However they are not different.\n",
    "# They are normiled to have unit length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Decompositions\n",
    "<a id=\"Matrix_Decompositions\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In linear algebra, matrix decomposition or matrix factorization is a factorization of a matrix into a product of matrices. Factorizing a matrix means that we want to find a product of matrices that is equal to the initial matrix. These techniques have a wide variety of uses and consequently, there exist several types of decompositions. Below, I will consider some of them, mostly applicable to machine learning or deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cholesky Decomposition\n",
    "<a id=\"Cholesky_Decomposition\"></a>\n",
    "***\n",
    "\n",
    "\n",
    "The Cholesky Decomposition is the factorization of a given **symmetric** square matrix $A$ into the product of a lower triangular matrix, denoted by $L$ and its transpose $L^{T}$. This decomposition is named after French artillery officer [Andre-Louis Cholesky](https://en.wikipedia.org/wiki/Andr%C3%A9-Louis_Cholesky). The formula is:\n",
    "\n",
    "\\begin{align*}A =\n",
    "LL^{T}\n",
    "\\end{align*}\n",
    "\n",
    "For rough sense, let $A$ be\n",
    "\n",
    "\\begin{align*}A =\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} & a_{13} \\\\\n",
    "a_{21} & a_{22} & a_{23} \\\\\n",
    "a_{31} & a_{32} & a_{33}\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Then we can represent $A$ as \n",
    "\n",
    "\\begin{align*}A = LL^{T} =\n",
    "\\begin{bmatrix}\n",
    "l_{11} & 0 & 0 \\\\\n",
    "l_{21} & l_{22} & 0 \\\\\n",
    "l_{31} & l_{32} & l_{33}\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "l_{11} & l_{12} & l_{13} \\\\\n",
    "0 & l_{22} & l_{23} \\\\\n",
    "0 & 0 & a_{33}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "l_{11}^{2} & l_{21}l_{11} & l_{31}l_{11} \\\\\n",
    "l_{21}l_{11} & l_{21}^{2} + l_{22}^{2} & l_{31}l_{21} + l_{32}l_{22} \\\\\n",
    "l_{31}l_{11} & l_{31}l_{21} + l_{32}l_{22} & l_{31}^{2} + l_{32}^{2} + l_{33}^2\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "The diagonal elements of matrix $L$ can be calculated by the following formulas:\n",
    "\n",
    "\\begin{align*}\n",
    "l_{11} = \\sqrt{a_{11}}\n",
    "\\quad \\quad\n",
    "l_{22} = \\sqrt{a_{22} - l_{21}^{2}}\n",
    "\\quad \\quad\n",
    "l_{33} = \\sqrt{a_{33} - (l_{31}^{2} + l_{32}{2})}\n",
    "\\end{align*}\n",
    "\n",
    "And in general, for diagonal elements of the matrix $L$ we have:\n",
    "\n",
    "\\begin{align*}l_{kk} =\n",
    "\\sqrt{a_{kk} - \\sum_{j = 1}^{k - 1}l_{kj}^{2}}\n",
    "\\end{align*}\n",
    "\n",
    "For the elements below the main diagonal, $l_{ik}$ where $i > k$, the formulas are\n",
    "\n",
    "\\begin{align*}\n",
    "l_{21} = \\frac{1}{l_{11}}a_{21}\n",
    "\\quad \\quad\n",
    "l_{31} = \\frac{1}{l_{11}}a_{31}\n",
    "\\quad \\quad\n",
    "l_{32} = \\frac{1}{l_{22}}(a_{32} - l_{31}l_{21})\n",
    "\\end{align*}\n",
    "\n",
    "And the general formula is\n",
    "\n",
    "\\begin{align*}l_{ik} =\n",
    "\\frac{1}{l_{kk}}\\Big(a_{ik} - \\sum_{j = 1}^{k - 1}l_{ij}l_{kj}\\Big)\n",
    "\\end{align*}\n",
    "\n",
    "Messy formulas! Consider a numerical example to see what happen under the hood. We have a matrix $A$\n",
    "\n",
    "\\begin{align*}A =\n",
    "\\begin{bmatrix}\n",
    "25 & 15 & -5 \\\\\n",
    "15 & 18 & 0 \\\\\n",
    "-5 & 0 & 11\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "According to the above formulas, let find a lower triangular matrix $L$. We have\n",
    "\n",
    "\\begin{align*}\n",
    "l_{11} = \\sqrt{a_{11}} = \\sqrt{25} = 5\n",
    "\\quad \\quad\n",
    "l_{22} = \\sqrt{a_{22} - l_{21}^{2}} = \\sqrt{18 - 3^{2}} = 3\n",
    "\\quad \\quad\n",
    "l_{33} = \\sqrt{a_{33} - (l_{31}^{2} + l_{32}^{2})} = \\sqrt{11 - ((-1)^{2} + 1^{2})} = 3\n",
    "\\end{align*}\n",
    "\n",
    "Seems, we have missing non-diagonal elements, which are\n",
    "\n",
    "\\begin{align*}\n",
    "l_{21} = \\frac{1}{l_{11}}a_{21} = \\frac{1}{5}15 = 3\n",
    "\\quad \\quad\n",
    "l_{31} = \\frac{1}{l_{11}}a_{31} = \\frac{1}{5}(-5) = -1\n",
    "\\quad \\quad\n",
    "l_{32} = \\frac{1}{l_{22}}(a_{32} - l_{31}l_{21}) = \\frac{1}{3}(0 - (-1)\\cdot 3) = 1\n",
    "\\end{align*}\n",
    "\n",
    "So, our matrix $L$ is\n",
    "\n",
    "\\begin{align*}L =\n",
    "\\begin{bmatrix}\n",
    "5 & 0 & 0 \\\\\n",
    "3 & 3 & 0 \\\\\n",
    "-1 & 1 & 3\n",
    "\\end{bmatrix}\n",
    "\\quad \\quad\n",
    "L^{T} =\n",
    "\\begin{bmatrix}\n",
    "5 & 3 & -1 \\\\\n",
    "0 & 3 & 1 \\\\\n",
    "0 & 0 & 3\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Multiplication of this matrices is up to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QR Decomposition\n",
    "<a id=\"QR_Decomposition\"></a>\n",
    "***\n",
    "\n",
    "\n",
    "QR decomposition is another type of matrix factorization, where a given $m \\times n$ matrix $A$ is decomposed into two matrices, $Q$ which is orthogonal matrix, which in turn means that $QQ^{T} = Q^{T}Q = I$ and the inverse of $Q$ equal to its transpose, $Q^{T} = Q^{-1}$, and $R$ which is upper triangular matrix. Hence, the formula is given by\n",
    "\n",
    "\\begin{align*}A = \n",
    "QR\n",
    "\\end{align*}\n",
    "\n",
    "As $Q$ is an orthogonal matrix, there are three methods to find $Q$, one is [Gramm-Schmidt Process](https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process), second is [Householder Transformation](https://en.wikipedia.org/wiki/Householder_transformation), and third is [Givens Rotation](https://en.wikipedia.org/wiki/Givens_rotation). These methods are out of the scope of this blog post series and hence I'm going to explain all of them in separate blog posts. Consequently, there is no calculation besides python code in numerical representation section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigendecomposition\n",
    "<a id=\"Eigendecomposition\"></a>\n",
    "***\n",
    "\n",
    "Here is the question. What's the usage of eigenvalues and eigenvectors? Besides other usages, they help us to perform matrix decomposition and this decomposition is called eigendecomposition or spectral decomposition. In the case of the eigendecomposition, we decompose the initial matrix into the product of its eigenvectors and eigenvalues by the following formula:\n",
    "\n",
    "\\begin{align*}\n",
    "A = Q \\Lambda Q^{-1}\n",
    "\\end{align*}\n",
    "\n",
    "$A$ is $n\\times n$ square matrix, $Q$ is the matrix whose columns are the eigenvectors, which in turn are linearly independent and $\\Lambda$ is diagonal matrix of eigenvalues of $A$ and these eigenvalues are not necessarily distinct.\n",
    "\n",
    "To see the detailed steps of this decomposition, consider the abovementioned example of the matrix $A$ for which we already found eigenvalues and eigenvectors.\n",
    "\n",
    "\\begin{align*}A =\n",
    "\\begin{bmatrix}\n",
    "2 & 0 & 0 \\\\\n",
    "0 & 3 & 4 \\\\\n",
    "0 & 4 & 9\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "Q =\n",
    "\\begin{bmatrix}\n",
    "0 & 1 & 0 \\\\\n",
    "-2 & 0 & 1 \\\\\n",
    "1 & 0 & 2\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "\\Lambda = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 2 & 0 \\\\\n",
    "0 & 0 & 11\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "Q^{-1} =\n",
    "\\begin{bmatrix}\n",
    "0 & -0.4 & 0.2 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 0.2 & 0.4\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "We have all the matrices and now take matrix multiplication according to the above formula. Particularly, multiply $Q$ by $\\Lambda$ and by $Q^{-1}$. We have to get original matrix $A$\n",
    "\n",
    "Furthermore, if matrix $A$ is a real symmetric matrix, then eigendecomposition can be performed by the following formula:\n",
    "\n",
    "\\begin{align*}\n",
    "A = Q \\Lambda Q^{T}\n",
    "\\end{align*}\n",
    "\n",
    "The only difference between this formula and above formula is that the matrix $A$ is $n\\times n$ real symmetric square matrix and instead of taking the inverse of eigenvector matrix we take the transpose of it. Moreover, for a real symmetric matrix, eigenvectors corresponding to different eigenvalues are orthogonal. Consider the following example:\n",
    "\n",
    "\\begin{align*}A =\n",
    "\\begin{bmatrix}\n",
    "6 & 2 \\\\\n",
    "2 & 3\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "The matrix is symmetric because of the original matrix equal to its transpose, $A = A^{T}$\n",
    "\n",
    "Its eigenvalues are $\\lambda_{1} = 7$ and $\\lambda_{2} = 2$ and corresponding eigenvectors are \n",
    "\n",
    "\\begin{align*}v_{\\lambda_{1}} =\n",
    "\\begin{bmatrix}\n",
    "0.89442719 \\\\\n",
    "0.4472136\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "v_{\\lambda_{2}} =\n",
    "\\begin{bmatrix}\n",
    "-0.4472136 \\\\\n",
    "0.89442719\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "And in this set up, matrices $Q$, $\\Lambda$ and $Q^{T}$ are the following:\n",
    "\n",
    "\\begin{align*}Q =\n",
    "\\begin{bmatrix}\n",
    "0.89442719 & -0.4472136 \\\\\n",
    "0.4472136 & 0.89442719 \\\\\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "\\Lambda = \n",
    "\\begin{bmatrix}\n",
    "7 & 0 \\\\\n",
    "0 & 2 \\\\\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "Q^{T} =\n",
    "\\begin{bmatrix}\n",
    "0.89442719 & 0.4472136 \\\\\n",
    "-0.4472136 & 0.89442719 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Taking matrix product gives initial matrix $A$.\n",
    "\n",
    "To verify all of this calculation see Python code below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eigendecomposition cannot be used for nonsquare matrices. Below, we will see the Singular Value Decomposition (SVD) which is another way of decomposing matrices. The advantage of the SVD is that you can use it also with non-square matrices.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition\n",
    "<a id=\"Singular_Value_Decomposition\"></a>\n",
    "***\n",
    "\n",
    "Singular Value Decomposition (SVD) is another way of matrix factorization. It is the generalization of the eigendecomposition. In this context, generalization means that eigendecomposition is applicable only for square $n \\times n$ matrices, while Singular Value Decomposition (SVD) is applicable for any $m \\times n$ matrices.\n",
    "\n",
    "SVD for a $m \\times n$ matrix $A$ is computed by the following formula:\n",
    "\n",
    "\\begin{align*}\n",
    "A = U \\ D \\ V^{T}\n",
    "\\end{align*}\n",
    "\n",
    "Where, $U$'s columns are *left singular vectors* of $A$, $V$'s columns are *right singular vectors* of $A$ and $D$  is a diagonal matrix, not necessarily square matrix, containing **singular values** of $A$ on main diagonal. Singular values of $m \\times n$ matrix $A$ are the **square roots of the eigenvalues** of $A^{T}A$, which is a square matrix. If our initial matrix $A$ is square or $n \\times n$ then singular values **coincide** eigenvalues. Moreover, all of these defines the path towards eigendecomposition. Let see how this path is defined.\n",
    "\n",
    "Matrices, $U$, $D$, and $V$ can be found by transforming $A$ into a square matrix and computing eigenvalues and eigenvectors of this transformed matrix. This transformation is done by multiplying $A$ by its transpose $A^{T}$. After that, matrices $U$, $D$ and $V$ are the following:\n",
    "\n",
    "* $U$ corresponds to the eigenvectors of $AA^{T}$\n",
    "\n",
    "* $V$ corresponds to eigenvectors of $A^{T}A$\n",
    "\n",
    "* $D$ corresponds to eigenvalues, either $AA^{T}$ or $A^{T}A$, which are the same\n",
    "\n",
    "Theory almost always seems confusing. Consider a numerical example and Python code below for clarification.\n",
    "\n",
    "Let our initial matrix $A$ be:\n",
    "\n",
    "\\begin{align*}A =\n",
    "\\begin{bmatrix}\n",
    "0 & 1 & 0 \\\\\n",
    "\\sqrt{2} & 2 & 0 \\\\\n",
    "0 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Here, to use SVD first we need to find $AA^{T}$ and $A^{T}A$.\n",
    "\n",
    "\\begin{align*}AA^{T} =\n",
    "\\begin{bmatrix}\n",
    "2 & 2 & 2 \\\\\n",
    "2 & 6 & 2 \\\\\n",
    "2 & 2 & 2\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "A^{T}A =\n",
    "\\begin{bmatrix}\n",
    "2 & 2\\sqrt{2} & 0 \\\\\n",
    "2\\sqrt{2} & 6 & 2 \\\\\n",
    "0 & 2 & 2\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "In the next step, we have to find eigenvalues and eigenvectors for $AA^{T}$ and $A^{T}A$. The characteristic polynomial is\n",
    "\n",
    "\\begin{align*}\n",
    "-\\lambda^{3} + 10\\lambda^2 - 16\\lambda\n",
    "\\end{align*}\n",
    "\n",
    "with roots equal to $\\lambda_{1} = 8$, $\\lambda_{2} = 2$, and $\\lambda_{3} = 0$. Note that these eigenvalues are the same for the $A^{T}A$. We need singular values which are square root from eigenvalues. Let denote them by $\\sigma$ such as $\\sigma_{1} = \\sqrt{8} = 2\\sqrt{2}$, $\\sigma_{2} = \\sqrt{2}$ and $\\sigma_{3} = \\sqrt{0} = 0$. We now can construct diagonal matrix of singular values:\n",
    "\n",
    "\\begin{align*}D =\n",
    "\\begin{bmatrix}\n",
    "2\\sqrt{2} & 0 & 0 \\\\\n",
    "0 & \\sqrt{2} & 0 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Now we have to find matrices $U$ and $V$. We have everything what we need. First find eigenvectors of $AA^{T}$ for $\\lambda_{1} = 8$, $\\lambda_{2} = 2$, and $\\lambda_{3} = 0$, which are the following:\n",
    "\n",
    "\\begin{align*}U_{1} =\n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{\\sqrt{6}}\\\\\n",
    "\\frac{2}{\\sqrt{6}} \\\\\n",
    "\\frac{1}{\\sqrt{6}}\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "U_{2} =\n",
    "\\begin{bmatrix}\n",
    "-\\frac{1}{\\sqrt{3}}\\\\\n",
    "\\frac{1}{\\sqrt{3}} \\\\\n",
    "-\\frac{1}{\\sqrt{3}}\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "U_{3} =\n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{\\sqrt{2}}\\\\\n",
    "0 \\\\\n",
    "-\\frac{1}{\\sqrt{2}}\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "Note that eigenvectors are normilized.\n",
    "\n",
    "As we have eigenvectors, our $U$ matrix is:\n",
    "\n",
    "\\begin{align*}U =\n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{2}}\\\\\n",
    "\\frac{2}{\\sqrt{6}} & \\frac{1}{\\sqrt{3}} & 0 \\\\\n",
    "\\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{3}} & -\\frac{1}{\\sqrt{2}}\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "In the same fashin, we can find matrix $V$, which is:\n",
    "\n",
    "\\begin{align*}V =\n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{\\sqrt{6}} & \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{2}}\\\\\n",
    "\\frac{3}{\\sqrt{12}} & 0 & -\\frac{1}{2} \\\\\n",
    "\\frac{1}{\\sqrt{12}} & -\\frac{2}{\\sqrt{6}} & \\frac{1}{2}\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "According to the formula we have\n",
    "\n",
    "\\begin{align*}\n",
    "A = U \\ D \\ V^{T} =\n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{2}}\\\\\n",
    "\\frac{2}{\\sqrt{6}} & \\frac{1}{\\sqrt{3}} & 0 \\\\\n",
    "\\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{3}} & -\\frac{1}{\\sqrt{2}}\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "2\\sqrt{2} & 0 & 0 \\\\\n",
    "0 & \\sqrt{2} & 0 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{\\sqrt{6}} & \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{2}}\\\\\n",
    "\\frac{3}{\\sqrt{12}} & 0 & -\\frac{1}{2} \\\\\n",
    "\\frac{1}{\\sqrt{12}} & -\\frac{2}{\\sqrt{6}} & \\frac{1}{2}\n",
    "\\end{bmatrix}\n",
    "^{T} = A\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse of a Square Full Rank Matrix\n",
    "<a id=\"Inverse_of_a_Square_Full_Rank_Matrix\"></a>\n",
    "***\n",
    "\n",
    "Here, I want to present one more way to find the inverse of a matrix and show you one more usage of eigendecomposition. Let's get started. If a matrix $A$ can be eigendecomposed and it has no any eigenvalue equal to zero, then this matrix has the inverse and this inverse is given by:\n",
    "\n",
    "\\begin{align*}A^{-1} =\n",
    "Q \\Lambda^{-1} Q^{-1}\n",
    "\\end{align*}\n",
    "\n",
    "Matrices, $Q$, and $\\Lambda$ are already known for us. Consider an example:\n",
    "\n",
    "\\begin{align*}A =\n",
    "\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "4 & 3\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Its eigenvalues are $\\lambda_{1} = -1$ and $\\lambda_{2} = 5$ and eigenvectors are:\n",
    "\n",
    "\\begin{align*}v_{\\lambda_{1}} =\n",
    "\\begin{bmatrix}\n",
    "-0.70710678 \\\\\n",
    "0.70710678\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "v_{\\lambda_{2}} =\n",
    "\\begin{bmatrix}\n",
    "0.4472136 \\\\\n",
    "-0.89442719\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Let calculate the onverse of $A$\n",
    "\n",
    "\\begin{align*}A^{-1} = Q \\Lambda^{-1} Q^{-1} =\n",
    "\\begin{bmatrix}\n",
    "-0.70710678 & -0.4472136 \\\\\n",
    "0.70710678 & -0.89442719\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "-1 & -0 \\\\\n",
    "0 & 0.2\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "-0.94280904 & 0.47140452 \\\\\n",
    "-0.74535599 & -0.74535599\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "-0.6 & 0.4 \\\\\n",
    "0.8 & -0.2\n",
    "\\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Representation\n",
    "<a id=\"Numerical_Representation_Decompositions\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cholesky Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "A = np.array([[25,15,-5],\n",
    "              [15,18,0],\n",
    "              [-5,0,11]])\n",
    "\n",
    "\n",
    "# Cholesky decomposition, find lower triangular matrix L\n",
    "L = np.linalg.cholesky(A)\n",
    "\n",
    "# Take transpose\n",
    "L_T = np.transpose(L)\n",
    "\n",
    "\n",
    "# Check if it's correct\n",
    "A == np.dot(L, L_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QR Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "A = np.array([[12,-51,4],\n",
    "              [6,167,-68],\n",
    "              [-4,24,-41]])\n",
    "\n",
    "\n",
    "# QR decomposition\n",
    "Q, R = np.linalg.qr(A)\n",
    "\n",
    "\n",
    "print(\"Q =\", Q, sep='\\n')\n",
    "print()\n",
    "print(\"R =\", R, sep='\\n')\n",
    "print()\n",
    "\n",
    "print(\"A = QR\", np.dot(Q,R), sep='\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eigendecomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Eigendecomposition for nonsymmetric matrix\n",
    "\n",
    "A = np.array([[2,0,0],\n",
    "              [0,3,4],\n",
    "              [0,4,9]])\n",
    "\n",
    "\n",
    "eigenvalues1, eigenvectors1 = np.linalg.eig(A)\n",
    "\n",
    "\n",
    "# Form diagonal matrix from eigenvalues\n",
    "L1 = np.diag(eigenvalues1)\n",
    "\n",
    "\n",
    "# Seperate eigenvector matrix and take its inverse\n",
    "Q1 = eigenvectors1\n",
    "inv_Q = np.linalg.inv(Q1)\n",
    "\n",
    "\n",
    "B = np.dot(np.dot(Q1,L1),inv_Q)\n",
    "\n",
    "\n",
    "# Check if B equal to A\n",
    "print(\"Decomposed matrix B:\")\n",
    "print(B)\n",
    "\n",
    "\n",
    "# Numpy produces normilized eigenvectors and don't be confused with my calculations above\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Eigendecomposition for symmetric matrix\n",
    "\n",
    "C = np.array([[6,2],[2,3]])\n",
    "\n",
    "eigenvalues2, eigenvectors2 = np.linalg.eig(C)\n",
    "\n",
    "# Eigenvalues\n",
    "L2 = np.diag(eigenvalues2)\n",
    "\n",
    "# Eigenvectors\n",
    "Q2 = eigenvectors2\n",
    "Q2_T = Q2.T\n",
    "\n",
    "\n",
    "D = np.dot(np.dot(Q2,L2),Q2.T)\n",
    "\n",
    "\n",
    "# Check if D equal to C\n",
    "print()\n",
    "print(\"Decomposed matrix D:\")\n",
    "print(D)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(suppress=True) # Suppres scientific notation\n",
    "\n",
    "A = np.array([[0,1,0],\n",
    "              [np.sqrt(2),2,0],\n",
    "              [0,1,1]])\n",
    "\n",
    "\n",
    "U, D, V = np.linalg.svd(A)\n",
    "print(\"U =\", U)\n",
    "print()\n",
    "print(\"D =\", D)\n",
    "print()\n",
    "print(\"V =\", V)\n",
    "\n",
    "B = np.dot(U, np.dot(np.diag(D), V))\n",
    "print()\n",
    "print(\"B =\", B)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inverse of a Square Full Rank Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "A = np.array([[1,2],\n",
    "              [4,3]])\n",
    "\n",
    "\n",
    "# Eigenvalues and Eigenvectors\n",
    "L, Q = np.linalg.eig(A)\n",
    "\n",
    "# Diagonal eigenvalues\n",
    "L = np.diag(L)\n",
    "# Inverse\n",
    "inv_L = np.linalg.inv(L)\n",
    "\n",
    "# Inverse of igenvector matrix\n",
    "inv_Q = np.linalg.inv(Q)\n",
    "\n",
    "\n",
    "# Calculate the inverse of A\n",
    "inv_A = np.dot(Q,np.dot(inv_L,inv_Q))\n",
    "\n",
    "# Print the inverse\n",
    "print(\"The inverse of A is\")\n",
    "print(inv_A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "<a id=\"Conclusion\"></a>\n",
    "***\n",
    "\n",
    "In conclusion, my aim was to make linear algebra tutorials which are in absence, while learning machine learning or deep learning. Particularly, existing materials either are pure mathematics books which cover lots of unnecessary(actually they are necessary) things or machine learning books which assume that you already have some linear algebra knowledge. The series starts from very basic and at the end explains some advanced topics. I can say that I tried my best to filter the materials and only explained the most relevant linear algebra topic for machine learning and deep learning.\n",
    "\n",
    "Based on my experience, these tutorials are not enough to master the concepts and all intuitions but the journey should be continuous. Meaning, that you have to practice more and more.\n",
    "\n",
    "Now, I realize that, for aspiring data scientists, besides my hard work, some topic may not be still clear. For that don't hesitate and comment below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "<a id=\"References\"></a>\n",
    "\n",
    "#### Vector\n",
    "\n",
    "- [Linear Algebra Done Right](http://linear.axler.net/)\n",
    "\n",
    "\n",
    "#### Matrix\n",
    "\n",
    "- [Linear Algebra Topics](https://en.wikipedia.org/wiki/List_of_linear_algebra_topics)\n",
    "\n",
    "\n",
    "#### Matrix Decomposition\n",
    "\n",
    "- [Cholesky Decomposition](https://rosettacode.org/wiki/Cholesky_decomposition)\n",
    "\n",
    "- [Matrix Decomposition](https://en.wikipedia.org/wiki/Matrix_decomposition)\n",
    "\n",
    "- [Introduction To Linear Algebra](http://math.mit.edu/~gs/linearalgebra/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
