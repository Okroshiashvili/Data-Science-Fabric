{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This is the fourth and last post in blog series about linear algebra. \n",
        "\n",
        "1. Introduction\n",
        "2. Basics of linear algebra\n",
        "3. Intermediate linear algebra\n",
        "4. **Advances in linear algebra**: Part I and **Part II**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the continuation of the part one and in this post I will introduce you the following topics:\n",
        "\n",
        "\n",
        "* [Matrix Decompositions](#Matrix_Decompositions)\n",
        "    * [Cholesky Decomposition](#Cholesky_Decomposition)\n",
        "    * [QR Decomposition](#QR_Decomposition)\n",
        "    * [Eigendecomposition](#Eigendecomposition)\n",
        "    * [Singular Value Decomposition](#Singular_Value_Decomposition)\n",
        "    * [Inverse of a Square Full Rank Matrix](#Inverse_of_a_Square_Full_Rank_Matrix)\n",
        "    * [Numerical Representation](#Numerical_Representation_Decompositions)\n",
        "* [Conclusion](#Conclusion)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matrix Decompositions\n",
        "<a id=\"Matrix_Decompositions\"></a>"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "In linear algebra, matrix decomposition or matrix factorization is a factorization of a matrix into a product of matrices. Factorizing a matrix means that we want to find a product of matrices that is equal to the initial matrix. These techniques have a wide variety of uses and consequently, there exist several types of decompositions. Below, I will consider some of them, mostly applicable to machine learning or deep learning."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cholesky Decomposition\n",
        "<a id=\"Cholesky_Decomposition\"></a>\n",
        "***\n",
        "\n",
        "\n",
        "The Cholesky Decomposition is the factorization of a given **symmetric** square matrix $A$ into the product of a lower triangular matrix, denoted by $L$ and its transpose $L^{T}$. This decomposition is named after French artillery officer [Andre-Louis Cholesky](https://en.wikipedia.org/wiki/Andr%C3%A9-Louis_Cholesky). The formula is:\n",
        "\n",
        "\\begin{align*}A =\n",
        "LL^{T}\n",
        "\\end{align*}\n",
        "\n",
        "For rough sense, let $A$ be\n",
        "\n",
        "\\begin{align}A =\n",
        "\\begin{bmatrix}\n",
        "a_{11} & a_{12} & a_{13} \\\\\n",
        "a_{21} & a_{22} & a_{23} \\\\\n",
        "a_{31} & a_{32} & a_{33}\n",
        "\\end{bmatrix}\n",
        "\\end{align}\n",
        "\n",
        "Then we can represent $A$ as \n",
        "\n",
        "\\begin{align*}A = LL^{T} =\n",
        "\\begin{bmatrix}\n",
        "l_{11} & 0 & 0 \\\\\n",
        "l_{21} & l_{22} & 0 \\\\\n",
        "l_{31} & l_{32} & l_{33}\n",
        "\\end{bmatrix}\n",
        "\\cdot\n",
        "\\begin{bmatrix}\n",
        "l_{11} & l_{12} & l_{13} \\\\\n",
        "0 & l_{22} & l_{23} \\\\\n",
        "0 & 0 & a_{33}\n",
        "\\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "l_{11}^{2} & l_{21}l_{11} & l_{31}l_{11} \\\\\n",
        "l_{21}l_{11} & l_{21}^{2} + l_{22}^{2} & l_{31}l_{21} + l_{32}l_{22} \\\\\n",
        "l_{31}l_{11} & l_{31}l_{21} + l_{32}l_{22} & l_{31}^{2} + l_{32}^{2} + l_{33}^2\n",
        "\\end{bmatrix}\n",
        "\\end{align*}\n",
        "\n",
        "The diagonal elements of matrix $L$ can be calculated by the following formulas:\n",
        "\n",
        "\\begin{align*}\n",
        "l_{11} = \\sqrt{a_{11}}\n",
        "\\quad \\quad\n",
        "l_{22} = \\sqrt{a_{22} - l_{21}^{2}}\n",
        "\\quad \\quad\n",
        "l_{33} = \\sqrt{a_{33} - (l_{31}^{2} + l_{32}{2})}\n",
        "\\end{align*}\n",
        "\n",
        "And in general, for diagonal elements of the matrix $L$ we have:\n",
        "\n",
        "\\begin{align*}l_{kk} =\n",
        "\\sqrt{a_{kk} - \\sum_{j = 1}^{k - 1}l_{kj}^{2}}\n",
        "\\end{align*}\n",
        "\n",
        "For the elements below the main diagonal, $l_{ik}$ where $i > k$, the formulas are\n",
        "\n",
        "\\begin{align*}\n",
        "l_{21} = \\frac{1}{l_{11}}a_{21}\n",
        "\\quad \\quad\n",
        "l_{31} = \\frac{1}{l_{11}}a_{31}\n",
        "\\quad \\quad\n",
        "l_{32} = \\frac{1}{l_{22}}(a_{32} - l_{31}l_{21})\n",
        "\\end{align*}\n",
        "\n",
        "And the general formula is\n",
        "\n",
        "\\begin{align*}l_{ik} =\n",
        "\\frac{1}{l_{kk}}\\Big(a_{ik} - \\sum_{j = 1}^{k - 1}l_{ij}l_{kj}\\Big)\n",
        "\\end{align*}\n",
        "\n",
        "Messy formulas! Consider a numerical example to see what happen under the hood. We have a matrix $A$\n",
        "\n",
        "\\begin{align*}A =\n",
        "\\begin{bmatrix}\n",
        "25 & 15 & -5 \\\\\n",
        "15 & 18 & 0 \\\\\n",
        "-5 & 0 & 11\n",
        "\\end{bmatrix}\n",
        "\\end{align*}\n",
        "\n",
        "According to the above formulas, let find a lower triangular matrix $L$. We have\n",
        "\n",
        "\\begin{align*}\n",
        "l_{11} = \\sqrt{a_{11}} = \\sqrt{25} = 5\n",
        "\\quad \\quad\n",
        "l_{22} = \\sqrt{a_{22} - l_{21}^{2}} = \\sqrt{18 - 3^{2}} = 3\n",
        "\\quad \\quad\n",
        "l_{33} = \\sqrt{a_{33} - (l_{31}^{2} + l_{32}^{2})} = \\sqrt{11 - ((-1)^{2} + 1^{2})} = 3\n",
        "\\end{align*}\n",
        "\n",
        "Seems, we have missing non-diagonal elements, which are\n",
        "\n",
        "\\begin{align*}\n",
        "l_{21} = \\frac{1}{l_{11}}a_{21} = \\frac{1}{5}15 = 3\n",
        "\\quad \\quad\n",
        "l_{31} = \\frac{1}{l_{11}}a_{31} = \\frac{1}{5}(-5) = -1\n",
        "\\quad \\quad\n",
        "l_{32} = \\frac{1}{l_{22}}(a_{32} - l_{31}l_{21}) = \\frac{1}{3}(0 - (-1)\\cdot 3) = 1\n",
        "\\end{align*}\n",
        "\n",
        "So, our matrix $L$ is\n",
        "\n",
        "\\begin{align*}L =\n",
        "\\begin{bmatrix}\n",
        "5 & 0 & 0 \\\\\n",
        "3 & 3 & 0 \\\\\n",
        "-1 & 1 & 3\n",
        "\\end{bmatrix}\n",
        "\\quad \\quad\n",
        "L^{T} =\n",
        "\\begin{bmatrix}\n",
        "5 & 3 & -1 \\\\\n",
        "0 & 3 & 1 \\\\\n",
        "0 & 0 & 3\n",
        "\\end{bmatrix}\n",
        "\\end{align*}\n",
        "\n",
        "Multiplication of this matrices is up to you."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### QR Decomposition\n",
        "<a id=\"QR_Decomposition\"></a>\n",
        "***\n",
        "\n",
        "\n",
        "QR decomposition is another type of matrix factorization, where a given $m \\times n$ matrix $A$ is decomposed into two matrices, $Q$ which is orthogonal matrix, which in turn means that $QQ^{T} = Q^{T}Q = I$ and the inverse of $Q$ equal to its transpose, $Q^{T} = Q^{-1}$, and $R$ which is upper triangular matrix. Hence, the formula is given by\n",
        "\n",
        "\\begin{align*}A = \n",
        "QR\n",
        "\\end{align*}\n",
        "\n",
        "As $Q$ is an orthogonal matrix, there are three methods to find $Q$, one is [Gramm-Schmidt Process](https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process), second is [Householder Transformation](https://en.wikipedia.org/wiki/Householder_transformation), and third is [Givens Rotation](https://en.wikipedia.org/wiki/Givens_rotation). These methods are out of the scope of this blog post series and hence I'm going to explain all of them in separate blog posts. Consequently, there is no calculation besides python code in numerical representation section."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Eigendecomposition\n",
        "<a id=\"Eigendecomposition\"></a>\n",
        "***\n",
        "\n",
        "Here is the question. What's the usage of eigenvalues and eigenvectors? Besides other usages, they help us to perform matrix decomposition and this decomposition is called eigendecomposition or spectral decomposition. In the case of the eigendecomposition, we decompose the initial matrix into the product of its eigenvectors and eigenvalues by the following formula:\n",
        "\n",
        "\\begin{align*}\n",
        "A = Q \\Lambda Q^{-1}\n",
        "\\end{align*}\n",
        "\n",
        "$A$ is $n\\times n$ square matrix, $Q$ is the matrix whose columns are the eigenvectors, which in turn are linearly independent and $\\Lambda$ is diagonal matrix of eigenvalues of $A$ and these eigenvalues are not necessarily distinct.\n",
        "\n",
        "To see the detailed steps of this decomposition, consider the abovementioned example of the matrix $A$ for which we already found eigenvalues and eigenvectors.\n",
        "\n",
        "\\begin{align*}A =\n",
        "\\begin{bmatrix}\n",
        "2 & 0 & 0 \\\\\n",
        "0 & 3 & 4 \\\\\n",
        "0 & 4 & 9\n",
        "\\end{bmatrix}\n",
        "\\quad\n",
        "Q =\n",
        "\\begin{bmatrix}\n",
        "0 & 1 & 0 \\\\\n",
        "-2 & 0 & 1 \\\\\n",
        "1 & 0 & 2\n",
        "\\end{bmatrix}\n",
        "\\quad\n",
        "\\Lambda = \n",
        "\\begin{bmatrix}\n",
        "1 & 0 & 0 \\\\\n",
        "0 & 2 & 0 \\\\\n",
        "0 & 0 & 11\n",
        "\\end{bmatrix}\n",
        "\\quad\n",
        "Q^{-1} =\n",
        "\\begin{bmatrix}\n",
        "0 & -0.4 & 0.2 \\\\\n",
        "1 & 0 & 0 \\\\\n",
        "0 & 0.2 & 0.4\n",
        "\\end{bmatrix}\n",
        "\\end{align*}\n",
        "\n",
        "We have all the matrices and now take matrix multiplication according to the above formula. Particularly, multiply $Q$ by $\\Lambda$ and by $Q^{-1}$. We have to get original matrix $A$\n",
        "\n",
        "Furthermore, if matrix $A$ is a real symmetric matrix, then eigendecomposition can be performed by the following formula:\n",
        "\n",
        "\\begin{align*}\n",
        "A = Q \\Lambda Q^{T}\n",
        "\\end{align*}\n",
        "\n",
        "The only difference between this formula and above formula is that the matrix $A$ is $n\\times n$ real symmetric square matrix and instead of taking the inverse of eigenvector matrix we take the transpose of it. Moreover, for a real symmetric matrix, eigenvectors corresponding to different eigenvalues are orthogonal. Consider the following example:\n",
        "\n",
        "\\begin{align*}A =\n",
        "\\begin{bmatrix}\n",
        "6 & 2 \\\\\n",
        "2 & 3\n",
        "\\end{bmatrix}\n",
        "\\end{align*}\n",
        "\n",
        "The matrix is symmetric because of the original matrix equal to its transpose, $A = A^{T}$\n",
        "\n",
        "Its eigenvalues are $\\lambda_{1} = 7$ and $\\lambda_{2} = 2$ and corresponding eigenvectors are \n",
        "\n",
        "\\begin{align*}v_{\\lambda_{1}} =\n",
        "\\begin{bmatrix}\n",
        "0.89442719 \\\\\n",
        "0.4472136\n",
        "\\end{bmatrix}\n",
        "\\quad\n",
        "v_{\\lambda_{2}} =\n",
        "\\begin{bmatrix}\n",
        "-0.4472136 \\\\\n",
        "0.89442719\n",
        "\\end{bmatrix}\n",
        "\\end{align*}\n",
        "\n",
        "And in this set up, matrices $Q$, $\\Lambda$ and $Q^{T}$ are the following:\n",
        "\n",
        "\\begin{align*}Q =\n",
        "\\begin{bmatrix}\n",
        "0.89442719 & -0.4472136 \\\\\n",
        "0.4472136 & 0.89442719 \\\\\n",
        "\\end{bmatrix}\n",
        "\\quad\n",
        "\\Lambda = \n",
        "\\begin{bmatrix}\n",
        "7 & 0 \\\\\n",
        "0 & 2 \\\\\n",
        "\\end{bmatrix}\n",
        "\\quad\n",
        "Q^{T} =\n",
        "\\begin{bmatrix}\n",
        "0.89442719 & 0.4472136 \\\\\n",
        "-0.4472136 & 0.89442719 \\\\\n",
        "\\end{bmatrix}\n",
        "\\end{align*}\n",
        "\n",
        "Taking matrix product gives initial matrix $A$.\n",
        "\n",
        "To verify all of this calculation see Python code below."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Eigendecomposition cannot be used for nonsquare matrices. Below, we will see the Singular Value Decomposition (SVD) which is another way of decomposing matrices. The advantage of the SVD is that you can use it also with non-square matrices.**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Singular Value Decomposition\n",
        "<a id=\"Singular_Value_Decomposition\"></a>\n",
        "***\n",
        "\n",
        "Singular Value Decomposition (SVD) is another way of matrix factorization. It is the generalization of the eigendecomposition. In this context, generalization means that eigendecomposition is applicable only for square $n \\times n$ matrices, while Singular Value Decomposition (SVD) is applicable for any $m \\times n$ matrices.\n",
        "\n",
        "SVD for a $m \\times n$ matrix $A$ is computed by the following formula:\n",
        "\n",
        "\\begin{align*}\n",
        "A = U \\ D \\ V^{T}\n",
        "\\end{align*}\n",
        "\n",
        "Where, $U$'s columns are *left singular vectors* of $A$, $V$'s columns are *right singular vectors* of $A$ and $D$  is a diagonal matrix, not necessarily square matrix, containing **singular values** of $A$ on main diagonal. Singular values of $m \\times n$ matrix $A$ are the **square roots of the eigenvalues** of $A^{T}A$, which is a square matrix. If our initial matrix $A$ is square or $n \\times n$ then singular values **coincide** eigenvalues. Moreover, all of these defines the path towards eigendecomposition. Let see how this path is defined.\n",
        "\n",
        "Matrices, $U$, $D$, and $V$ can be found by transforming $A$ into a square matrix and computing eigenvalues and eigenvectors of this transformed matrix. This transformation is done by multiplying $A$ by its transpose $A^{T}$. After that, matrices $U$, $D$ and $V$ are the following:\n",
        "\n",
        "* $U$ corresponds to the eigenvectors of $AA^{T}$\n",
        "\n",
        "* $V$ corresponds to eigenvectors of $A^{T}A$\n",
        "\n",
        "* $D$ corresponds to eigenvalues, either $AA^{T}$ or $A^{T}A$, which are the same\n",
        "\n",
        "Theory almost always seems confusing. Consider a numerical example and Python code below for clarification.\n",
        "\n",
        "Let our initial matrix $A$ be:\n",
        "\n",
        "\\begin{align*}A =\n",
        "\\begin{bmatrix}\n",
        "0 & 1 & 0 \\\\\n",
        "\\sqrt{2} & 2 & 0 \\\\\n",
        "0 & 1 & 1\n",
        "\\end{bmatrix}\n",
        "\\end{align*}\n",
        "\n",
        "Here, to use SVD first we need to find $AA^{T}$ and $A^{T}A$.\n",
        "\n",
        "\\begin{align*}AA^{T} =\n",
        "\\begin{bmatrix}\n",
        "2 & 2 & 2 \\\\\n",
        "2 & 6 & 2 \\\\\n",
        "2 & 2 & 2\n",
        "\\end{bmatrix}\n",
        "\\quad\n",
        "A^{T}A =\n",
        "\\begin{bmatrix}\n",
        "2 & 2\\sqrt{2} & 0 \\\\\n",
        "2\\sqrt{2} & 6 & 2 \\\\\n",
        "0 & 2 & 2\n",
        "\\end{bmatrix}\n",
        "\\end{align*}\n",
        "\n",
        "In the next step, we have to find eigenvalues and eigenvectors for $AA^{T}$ and $A^{T}A$. The characteristic polynomial is\n",
        "\n",
        "\\begin{align*}\n",
        "-\\lambda^{3} + 10\\lambda^2 - 16\\lambda\n",
        "\\end{align*}\n",
        "\n",
        "with roots equal to $\\lambda_{1} = 8$, $\\lambda_{2} = 2$, and $\\lambda_{3} = 0$. Note that these eigenvalues are the same for the $A^{T}A$. We need singular values which are square root from eigenvalues. Let denote them by $\\sigma$ such as $\\sigma_{1} = \\sqrt{8} = 2\\sqrt{2}$, $\\sigma_{2} = \\sqrt{2}$ and $\\sigma_{3} = \\sqrt{0} = 0$. We now can construct diagonal matrix of singular values:\n",
        "\n",
        "\\begin{align*}D =\n",
        "\\begin{bmatrix}\n",
        "2\\sqrt{2} & 0 & 0 \\\\\n",
        "0 & \\sqrt{2} & 0 \\\\\n",
        "0 & 0 & 0\n",
        "\\end{bmatrix}\n",
        "\\end{align*}\n",
        "\n",
        "Now we have to find matrices $U$ and $V$. We have everything what we need. First find eigenvectors of $AA^{T}$ for $\\lambda_{1} = 8$, $\\lambda_{2} = 2$, and $\\lambda_{3} = 0$, which are the following:\n",
        "\n",
        "\\begin{align*}U_{1} =\n",
        "\\begin{bmatrix}\n",
        "\\frac{1}{\\sqrt{6}}\\\\\n",
        "\\frac{2}{\\sqrt{6}} \\\\\n",
        "\\frac{1}{\\sqrt{6}}\n",
        "\\end{bmatrix}\n",
        "\\quad\n",
        "U_{2} =\n",
        "\\begin{bmatrix}\n",
        "-\\frac{1}{\\sqrt{3}}\\\\\n",
        "\\frac{1}{\\sqrt{3}} \\\\\n",
        "-\\frac{1}{\\sqrt{3}}\n",
        "\\end{bmatrix}\n",
        "\\quad\n",
        "U_{3} =\n",
        "\\begin{bmatrix}\n",
        "\\frac{1}{\\sqrt{2}}\\\\\n",
        "0 \\\\\n",
        "-\\frac{1}{\\sqrt{2}}\n",
        "\\end{bmatrix}\n",
        "\\end{align*}\n",
        "Note that eigenvectors are normilized.\n",
        "\n",
        "As we have eigenvectors, our $U$ matrix is:\n",
        "\n",
        "\\begin{align*}U =\n",
        "\\begin{bmatrix}\n",
        "\\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{2}}\\\\\n",
        "\\frac{2}{\\sqrt{6}} & \\frac{1}{\\sqrt{3}} & 0 \\\\\n",
        "\\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{3}} & -\\frac{1}{\\sqrt{2}}\n",
        "\\end{bmatrix}\n",
        "\\end{align*}\n",
        "\n",
        "In the same fashin, we can find matrix $V$, which is:\n",
        "\n",
        "\\begin{align*}V =\n",
        "\\begin{bmatrix}\n",
        "\\frac{1}{\\sqrt{6}} & \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{2}}\\\\\n",
        "\\frac{3}{\\sqrt{12}} & 0 & -\\frac{1}{2} \\\\\n",
        "\\frac{1}{\\sqrt{12}} & -\\frac{2}{\\sqrt{6}} & \\frac{1}{2}\n",
        "\\end{bmatrix}\n",
        "\\end{align*}\n",
        "\n",
        "According to the formula we have\n",
        "\n",
        "\\begin{align*}\n",
        "A = U \\ D \\ V^{T} =\n",
        "\\begin{bmatrix}\n",
        "\\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{2}}\\\\\n",
        "\\frac{2}{\\sqrt{6}} & \\frac{1}{\\sqrt{3}} & 0 \\\\\n",
        "\\frac{1}{\\sqrt{6}} & -\\frac{1}{\\sqrt{3}} & -\\frac{1}{\\sqrt{2}}\n",
        "\\end{bmatrix}\n",
        "\\cdot\n",
        "\\begin{bmatrix}\n",
        "2\\sqrt{2} & 0 & 0 \\\\\n",
        "0 & \\sqrt{2} & 0 \\\\\n",
        "0 & 0 & 0\n",
        "\\end{bmatrix}\n",
        "\\cdot\n",
        "\\begin{bmatrix}\n",
        "\\frac{1}{\\sqrt{6}} & \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{2}}\\\\\n",
        "\\frac{3}{\\sqrt{12}} & 0 & -\\frac{1}{2} \\\\\n",
        "\\frac{1}{\\sqrt{12}} & -\\frac{2}{\\sqrt{6}} & \\frac{1}{2}\n",
        "\\end{bmatrix}\n",
        "^{T} = A\n",
        "\\end{align*}"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inverse of a Square Full Rank Matrix\n",
        "<a id=\"Inverse_of_a_Square_Full_Rank_Matrix\"></a>\n",
        "***\n",
        "\n",
        "Here, I want to present one more way to find the inverse of a matrix and show you one more usage of eigendecomposition. Let's get started. If a matrix $A$ can be eigendecomposed and it has no any eigenvalue equal to zero, then this matrix has the inverse and this inverse is given by:\n",
        "\n",
        "\\begin{align*}A^{-1} =\n",
        "Q \\Lambda^{-1} Q^{-1}\n",
        "\\end{align*}\n",
        "\n",
        "Matrices, $Q$, and $\\Lambda$ are already known for us. Consider an example:\n",
        "\n",
        "\\begin{align*}A =\n",
        "\\begin{bmatrix}\n",
        "1 & 2 \\\\\n",
        "4 & 3\n",
        "\\end{bmatrix}\n",
        "\\end{align*}\n",
        "\n",
        "Its eigenvalues are $\\lambda_{1} = -1$ and $\\lambda_{2} = 5$ and eigenvectors are:\n",
        "\n",
        "\\begin{align*}v_{\\lambda_{1}} =\n",
        "\\begin{bmatrix}\n",
        "-0.70710678 \\\\\n",
        "0.70710678\n",
        "\\end{bmatrix}\n",
        "\\quad\n",
        "v_{\\lambda_{2}} =\n",
        "\\begin{bmatrix}\n",
        "0.4472136 \\\\\n",
        "-0.89442719\n",
        "\\end{bmatrix}\n",
        "\\end{align*}\n",
        "\n",
        "Let calculate the onverse of $A$\n",
        "\n",
        "\\begin{align*}A^{-1} = Q \\Lambda^{-1} Q^{-1} =\n",
        "\\begin{bmatrix}\n",
        "-0.70710678 & -0.4472136 \\\\\n",
        "0.70710678 & -0.89442719\n",
        "\\end{bmatrix}\n",
        "\\cdot\n",
        "\\begin{bmatrix}\n",
        "-1 & -0 \\\\\n",
        "0 & 0.2\n",
        "\\end{bmatrix}\n",
        "\\cdot\n",
        "\\begin{bmatrix}\n",
        "-0.94280904 & 0.47140452 \\\\\n",
        "-0.74535599 & -0.74535599\n",
        "\\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "-0.6 & 0.4 \\\\\n",
        "0.8 & -0.2\n",
        "\\end{bmatrix}\n",
        "\\end{align*}"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Numerical Representation\n",
        "<a id=\"Numerical_Representation_Decompositions\"></a>"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cholesky Decomposition"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "A = np.array([[25,15,-5],\n",
        "              [15,18,0],\n",
        "              [-5,0,11]])\n",
        "\n",
        "\n",
        "# Cholesky decomposition, find lower triangular matrix L\n",
        "L = np.linalg.cholesky(A)\n",
        "\n",
        "# Take transpose\n",
        "L_T = np.transpose(L)\n",
        "\n",
        "\n",
        "# Check if it's correct\n",
        "A == np.dot(L, L_T)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 1,
          "data": {
            "text/plain": [
              "array([[ True,  True,  True],\n",
              "       [ True,  True,  True],\n",
              "       [ True,  True,  True]])"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### QR Decomposition"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "A = np.array([[12,-51,4],\n",
        "              [6,167,-68],\n",
        "              [-4,24,-41]])\n",
        "\n",
        "\n",
        "# QR decomposition\n",
        "Q, R = np.linalg.qr(A)\n",
        "\n",
        "\n",
        "print(\"Q =\", Q, sep='\\n')\n",
        "print()\n",
        "print(\"R =\", R, sep='\\n')\n",
        "print()\n",
        "\n",
        "print(\"A = QR\", np.dot(Q,R), sep='\\n')\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q =\n",
            "[[-0.85714286  0.39428571  0.33142857]\n",
            " [-0.42857143 -0.90285714 -0.03428571]\n",
            " [ 0.28571429 -0.17142857  0.94285714]]\n",
            "\n",
            "R =\n",
            "[[ -14.  -21.   14.]\n",
            " [   0. -175.   70.]\n",
            " [   0.    0.  -35.]]\n",
            "\n",
            "A = QR\n",
            "[[ 12. -51.   4.]\n",
            " [  6. 167. -68.]\n",
            " [ -4.  24. -41.]]\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Eigendecomposition"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Eigendecomposition for nonsymmetric matrix\n",
        "\n",
        "A = np.array([[2,0,0],\n",
        "              [0,3,4],\n",
        "              [0,4,9]])\n",
        "\n",
        "\n",
        "eigenvalues1, eigenvectors1 = np.linalg.eig(A)\n",
        "\n",
        "\n",
        "# Form diagonal matrix from eigenvalues\n",
        "L1 = np.diag(eigenvalues1)\n",
        "\n",
        "\n",
        "# Seperate eigenvector matrix and take its inverse\n",
        "Q1 = eigenvectors1\n",
        "inv_Q = np.linalg.inv(Q1)\n",
        "\n",
        "\n",
        "B = np.dot(np.dot(Q1,L1),inv_Q)\n",
        "\n",
        "\n",
        "# Check if B equal to A\n",
        "print(\"Decomposed matrix B:\")\n",
        "print(B)\n",
        "\n",
        "\n",
        "# Numpy produces normilized eigenvectors and don't be confused with my calculations above\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Eigendecomposition for symmetric matrix\n",
        "\n",
        "C = np.array([[6,2],[2,3]])\n",
        "\n",
        "eigenvalues2, eigenvectors2 = np.linalg.eig(C)\n",
        "\n",
        "# Eigenvalues\n",
        "L2 = np.diag(eigenvalues2)\n",
        "\n",
        "# Eigenvectors\n",
        "Q2 = eigenvectors2\n",
        "Q2_T = Q2.T\n",
        "\n",
        "\n",
        "D = np.dot(np.dot(Q2,L2),Q2.T)\n",
        "\n",
        "\n",
        "# Check if D equal to C\n",
        "print()\n",
        "print(\"Decomposed matrix D:\")\n",
        "print(D)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decomposed matrix B:\n",
            "[[2. 0. 0.]\n",
            " [0. 3. 4.]\n",
            " [0. 4. 9.]]\n",
            "\n",
            "Decomposed matrix D:\n",
            "[[6. 2.]\n",
            " [2. 3.]]\n"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Singular Value Decomposition"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "np.set_printoptions(suppress=True) # Suppres scientific notation\n",
        "\n",
        "A = np.array([[0,1,0],\n",
        "              [np.sqrt(2),2,0],\n",
        "              [0,1,1]])\n",
        "\n",
        "\n",
        "U, D, V = np.linalg.svd(A)\n",
        "print(\"U =\", U)\n",
        "print()\n",
        "print(\"D =\", D)\n",
        "print()\n",
        "print(\"V =\", V)\n",
        "\n",
        "B = np.dot(U, np.dot(np.diag(D), V))\n",
        "print()\n",
        "print(\"B =\", B)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "U = [[-0.32099833  0.14524317 -0.93587632]\n",
            " [-0.87192053 -0.43111301  0.23215547]\n",
            " [-0.36974946  0.8905313   0.26502706]]\n",
            "\n",
            "D = [2.75398408 1.09310654 0.46977627]\n",
            "\n",
            "V = [[-0.44774472 -0.8840243  -0.13425984]\n",
            " [-0.55775521  0.15876626  0.81467932]\n",
            " [ 0.69888038 -0.43965249  0.56415592]]\n",
            "\n",
            "B = [[ 0.          1.         -0.        ]\n",
            " [ 1.41421356  2.          0.        ]\n",
            " [ 0.          1.          1.        ]]\n"
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Inverse of a Square Full Rank Matrix"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "A = np.array([[1,2],\n",
        "              [4,3]])\n",
        "\n",
        "\n",
        "# Eigenvalues and Eigenvectors\n",
        "L, Q = np.linalg.eig(A)\n",
        "\n",
        "# Diagonal eigenvalues\n",
        "L = np.diag(L)\n",
        "# Inverse\n",
        "inv_L = np.linalg.inv(L)\n",
        "\n",
        "# Inverse of igenvector matrix\n",
        "inv_Q = np.linalg.inv(Q)\n",
        "\n",
        "\n",
        "# Calculate the inverse of A\n",
        "inv_A = np.dot(Q,np.dot(inv_L,inv_Q))\n",
        "\n",
        "# Print the inverse\n",
        "print(\"The inverse of A is\")\n",
        "print(inv_A)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The inverse of A is\n",
            "[[-0.6  0.4]\n",
            " [ 0.8 -0.2]]\n"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "<a id=\"Conclusion\"></a>\n",
        "***\n",
        "\n",
        "In conclusion, my aim was to make linear algebra tutorials which are in absence, while learning machine learning or deep learning. Particularly, existing materials either are pure mathematics books which cover lots of unnecessary(actually they are necessary) things or machine learning books which assume that you already have some linear algebra knowledge. The series starts from very basic and at the end explains some advanced topics. I can say that I tried my best to filter the materials and only explained the most relevant linear algebra topics for machine learning and deep learning.\n",
        "\n",
        "Based on my experience, these tutorials are not enough to master the concepts and all intuitions but the journey should be continuous. Meaning, that you have to practice more and more.\n",
        "\n",
        "Now, I realize that, for aspiring data scientists, besides my hard work, some topic may not be still clear. For that don't hesitate and comment below. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### References\n",
        "<a id=\"References\"></a>\n",
        "\n",
        "\n",
        "#### Matrix Decomposition\n",
        "\n",
        "- [Cholesky Decomposition](https://rosettacode.org/wiki/Cholesky_decomposition)\n",
        "\n",
        "- [Matrix Decomposition](https://en.wikipedia.org/wiki/Matrix_decomposition)\n",
        "\n",
        "- [Introduction To Linear Algebra](http://math.mit.edu/~gs/linearalgebra/)"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.3",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "0.14.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}